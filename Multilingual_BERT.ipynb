{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fdb86eec",
      "metadata": {
        "id": "fdb86eec"
      },
      "source": [
        "# Multilingual BERT and Zero-Shot Transfer\n",
        "\n",
        "Fine-tuning a multilingual BERT or mBERT model on a Natural Language Inference task [XNLI](https://arxiv.org/abs/1809.05053). \n",
        "The model has been fine-tuned on English Training data and then the performance of the fine-tuned model has been evaluated on different languages demonstrating the zero-shot capabilities of mBERT. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cb7d9310",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cb7d9310",
        "outputId": "282a0a16-08fb-4504-9b40-d17f42f1e121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    data_dir = \"gdrive/MyDrive/PlakshaNLP/data/xnli\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8f1c0d05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8f1c0d05",
        "outputId": "0acc3d87-8fa0-4d27-ee35-dee51fd51839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "# !pip install numpy\n",
        "# !pip install pandas\n",
        "# !pip install torch\n",
        "# !pip install tqdm\n",
        "# !pip install matplotlib\n",
        "!pip install transformers\n",
        "# !pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "382573bd",
      "metadata": {
        "id": "382573bd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import copy\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "222e4a45",
      "metadata": {
        "id": "222e4a45"
      },
      "source": [
        "## XNLI: Task Description\n",
        "\n",
        "XNLI is a multilingual benchmark for Natural Language Inference, that contains training data available in English which was obtained from the popular [MNLI](https://cims.nyu.edu/~sbowman/multinli/), and test and dev sets available for 15 different languages. \n",
        "\n",
        "In NLI, we are given two sentences, one is a premise and other an hypothesis, and the task is to predict whether the hypothesis is i) entialed in the premise, or ii) contradicts the premise, or iii) neutral to the premise. \n",
        "\n",
        "<img src=\"https://i.ibb.co/bd4P20K/nli-examples.jpg\" alt=\"nli-examples\" border=\"0\">\n",
        "\n",
        "This makes NLI a multi-class classification task where we want to predict the correct label out of the three possible classes. We start by loading the dataset into memory. The training set in XNLI is comparitively huge with around 400k examples, which can lead to higher training times. Hence for the purpose of this assignment we will work with a fraction of the full data i.e. ~40k examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "40cf1fe5",
      "metadata": {
        "id": "40cf1fe5"
      },
      "outputs": [],
      "source": [
        "def load_xnli_dataset(lang, split = \"train\"):\n",
        "    filename = os.path.join(data_dir, f\"{split}-{lang}.tsv\")\n",
        "    sentence1s = []\n",
        "    sentence2s = []\n",
        "    labels = []\n",
        "    with open(filename) as f:\n",
        "        for i,line in enumerate(f):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            row = line.split(\"\\t\")\n",
        "            sentence1 = row[0]\n",
        "            sentence2 = row[1]\n",
        "            label = row[2].split(\"\\n\")[0]\n",
        "            sentence1s.append(sentence1)\n",
        "            sentence2s.append(sentence2)\n",
        "            labels.append((label))\n",
        "    \n",
        "    return pd.DataFrame({\n",
        "        \"premise\": sentence1s,\n",
        "        \"hypothesis\" : sentence2s,\n",
        "        \"label\" : labels\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1d0b9f05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "1d0b9f05",
        "outputId": "5a8ae080-b53c-46c2-94a4-d78984e69c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of examples in training data: 38000\n",
            "Number of examples in validation data: 2000\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7b7cdc2b-3124-4116-930c-44e87f386f58\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1294</th>\n",
              "      <td>A ten-minute walk east on Belford Road brings ...</td>\n",
              "      <td>There are at least two art galleries on Belfor...</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13368</th>\n",
              "      <td>But here he shook his head in answer to his ow...</td>\n",
              "      <td>He nodded his head to his own thoughts .</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>Where 's Tommy ?</td>\n",
              "      <td>Do you know where Tommy is ?</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32777</th>\n",
              "      <td>Its bell tolled the signal in 1572 for Catholi...</td>\n",
              "      <td>The St. Bartholomew Day massacre saw the killi...</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26170</th>\n",
              "      <td>yeah most of them do they have the uh yeah the...</td>\n",
              "      <td>I use the miles per hour more than the kilomet...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b7cdc2b-3124-4116-930c-44e87f386f58')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7b7cdc2b-3124-4116-930c-44e87f386f58 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7b7cdc2b-3124-4116-930c-44e87f386f58');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                 premise  \\\n",
              "1294   A ten-minute walk east on Belford Road brings ...   \n",
              "13368  But here he shook his head in answer to his ow...   \n",
              "308                                     Where 's Tommy ?   \n",
              "32777  Its bell tolled the signal in 1572 for Catholi...   \n",
              "26170  yeah most of them do they have the uh yeah the...   \n",
              "\n",
              "                                              hypothesis          label  \n",
              "1294   There are at least two art galleries on Belfor...     entailment  \n",
              "13368           He nodded his head to his own thoughts .  contradiction  \n",
              "308                         Do you know where Tommy is ?     entailment  \n",
              "32777  The St. Bartholomew Day massacre saw the killi...  contradiction  \n",
              "26170  I use the miles per hour more than the kilomet...        neutral  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load Training data in english\n",
        "train_en_data = load_xnli_dataset(\"en\", \"train\")[:40000]\n",
        "\n",
        "# We will split the training data to get some validation examples as well\n",
        "train_en_data, val_en_data = train_test_split(train_en_data, test_size=0.05)\n",
        "\n",
        "print(f\"Number of examples in training data: {len(train_en_data)}\")\n",
        "print(f\"Number of examples in validation data: {len(val_en_data)}\")\n",
        "\n",
        "train_en_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "96957733",
      "metadata": {
        "id": "96957733"
      },
      "outputs": [],
      "source": [
        "# Load Test data in other languages\n",
        "test_langs = [\"ar\", \"bg\", \"de\", \"el\", \"en\", \"es\", \"fr\", \"hi\", \"ru\", \"sw\", \"th\", \"tr\", \"ur\", \"vi\", \"zh\"]\n",
        "\n",
        "lang2test_df = {lang : load_xnli_dataset(lang, \"dev\") for lang in test_langs}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d8bc71b7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "d8bc71b7",
        "outputId": "83db9404-53c2-40d6-ddbf-d0e1f2e10455"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Test examples: 2489\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-598f7cee-df3b-46c1-81a6-b863f176b1b4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>And he said, Mama, I'm home.</td>\n",
              "      <td>He didn't say a word.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>And he said, Mama, I'm home.</td>\n",
              "      <td>He told his mom he had gotten home.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I didn't know what I was going for or anything...</td>\n",
              "      <td>I have never been to Washington so when I was ...</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I didn't know what I was going for or anything...</td>\n",
              "      <td>I knew exactly what I needed to do as I marche...</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I didn't know what I was going for or anything...</td>\n",
              "      <td>I was not quite certain what I was going to do...</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-598f7cee-df3b-46c1-81a6-b863f176b1b4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-598f7cee-df3b-46c1-81a6-b863f176b1b4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-598f7cee-df3b-46c1-81a6-b863f176b1b4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             premise  \\\n",
              "0                       And he said, Mama, I'm home.   \n",
              "1                       And he said, Mama, I'm home.   \n",
              "2  I didn't know what I was going for or anything...   \n",
              "3  I didn't know what I was going for or anything...   \n",
              "4  I didn't know what I was going for or anything...   \n",
              "\n",
              "                                          hypothesis          label  \n",
              "0                              He didn't say a word.  contradiction  \n",
              "1                He told his mom he had gotten home.     entailment  \n",
              "2  I have never been to Washington so when I was ...        neutral  \n",
              "3  I knew exactly what I needed to do as I marche...  contradiction  \n",
              "4  I was not quite certain what I was going to do...     entailment  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Number of Test examples: {len(lang2test_df['en'])}\")\n",
        "lang2test_df[\"en\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "362ebef0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "362ebef0",
        "outputId": "fc7ee633-a5d8-4f48-8673-9a766fd98c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ar test set:\n",
            "                                             premise  \\\n",
            "0                        وقال، ماما، لقد عدت للمنزل.   \n",
            "1                        وقال، ماما، لقد عدت للمنزل.   \n",
            "2  لم أعرف من أجل ماذا أنا ذاهب أو أي شىْ ، لذلك ...   \n",
            "3  لم أعرف من أجل ماذا أنا ذاهب أو أي شىْ ، لذلك ...   \n",
            "4  لم أعرف من أجل ماذا أنا ذاهب أو أي شىْ ، لذلك ...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                                  لم ينطق ببنت شفة.  contradiction  \n",
            "1                        أخبر أمه أنه قد عاد للمنزل.     entailment  \n",
            "2  لم أذهب إلى واشنطن من قبل، لذا عندما تم تكليفي...        neutral  \n",
            "3  لقد عرفت بالضبط ما الذي احتجت أن أفعله عندما م...  contradiction  \n",
            "4  لم أكن متأكدًا مما سأفعله لذلك ذهبت إلى واشنطن...     entailment  \n",
            "***************************\n",
            "\n",
            "bg test set:\n",
            "                                             premise  \\\n",
            "0                      И той каза: Мамо, у дома съм.   \n",
            "1                      И той каза: Мамо, у дома съм.   \n",
            "2  Не знаех за какво отивам и въобще нищо, но тря...   \n",
            "3  Не знаех за какво отивам и въобще нищо, но тря...   \n",
            "4  Не знаех за какво отивам и въобще нищо, но тря...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                             Той не каза нито дума.  contradiction  \n",
            "1       Той каза на майка си, че се е прибрал вкъщи.     entailment  \n",
            "2  Никога не съм бил във Вашингтон и когато ме на...        neutral  \n",
            "3  Знаех точно какво да направя когато отивах към...  contradiction  \n",
            "4  Не бях съвсем сигурен какво ще направя, така ч...     entailment  \n",
            "***************************\n",
            "\n",
            "de test set:\n",
            "                                             premise  \\\n",
            "0            und er hat gesagt, Mama ich bin daheim.   \n",
            "1            und er hat gesagt, Mama ich bin daheim.   \n",
            "2  Ich wusste nicht was ich vorhatte oder so, ich...   \n",
            "3  Ich wusste nicht was ich vorhatte oder so, ich...   \n",
            "4  Ich wusste nicht was ich vorhatte oder so, ich...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                                Er sagte kein Wort.  contradiction  \n",
            "1  Er sagte seiner Mutter, er sei nach Hause geko...     entailment  \n",
            "2  Ich war noch nie in Washington, deshalb habe i...        neutral  \n",
            "3  Ich wusste genau, was ich tun musste, als ich ...  contradiction  \n",
            "4  Ich war mir nicht ganz sicher was ich tun soll...     entailment  \n",
            "***************************\n",
            "\n",
            "el test set:\n",
            "                                             premise  \\\n",
            "0                  Και είπε, Μαμά, έφτασα στο σπίτι.   \n",
            "1                  Και είπε, Μαμά, έφτασα στο σπίτι.   \n",
            "2  Δεν ήξερα που πήγαινα ή κάτι τέτοιο, έτσι έπρε...   \n",
            "3  Δεν ήξερα που πήγαινα ή κάτι τέτοιο, έτσι έπρε...   \n",
            "4  Δεν ήξερα που πήγαινα ή κάτι τέτοιο, έτσι έπρε...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                                Δεν είπε ούτε λέξη.  contradiction  \n",
            "1            Είπε στην μαμά του ότι είχε πάει σπίτι.     entailment  \n",
            "2  Ποτέ δεν πήγα στην Ουάσινγκτον, οπότε όταν με ...        neutral  \n",
            "3  Ήξερα ακριβώς τι χρειαζόμουν καθώς όδευα προς ...  contradiction  \n",
            "4  Δεν ήμουν αρκετά σίγουρος για το τι θα κάνω, έ...     entailment  \n",
            "***************************\n",
            "\n",
            "en test set:\n",
            "                                             premise  \\\n",
            "0                       And he said, Mama, I'm home.   \n",
            "1                       And he said, Mama, I'm home.   \n",
            "2  I didn't know what I was going for or anything...   \n",
            "3  I didn't know what I was going for or anything...   \n",
            "4  I didn't know what I was going for or anything...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                              He didn't say a word.  contradiction  \n",
            "1                He told his mom he had gotten home.     entailment  \n",
            "2  I have never been to Washington so when I was ...        neutral  \n",
            "3  I knew exactly what I needed to do as I marche...  contradiction  \n",
            "4  I was not quite certain what I was going to do...     entailment  \n",
            "***************************\n",
            "\n",
            "es test set:\n",
            "                                             premise  \\\n",
            "0                    Y él dijo: Mamá, estoy en casa.   \n",
            "1                    Y él dijo: Mamá, estoy en casa.   \n",
            "2  No sabía para qué iba ni nada, así que iba a i...   \n",
            "3  No sabía para qué iba ni nada, así que iba a i...   \n",
            "4  No sabía para qué iba ni nada, así que iba a i...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                            Él no dijo una palabra.  contradiction  \n",
            "1       Le dijo a su madre que había llegado a casa.     entailment  \n",
            "2  Nunca he estado en Washington, así que cuando ...        neutral  \n",
            "3  Sabía exactamente lo que tenía que hacer mient...  contradiction  \n",
            "4  No estaba muy seguro de lo que iba a hacer, as...     entailment  \n",
            "***************************\n",
            "\n",
            "fr test set:\n",
            "                                             premise  \\\n",
            "0           Et il a dit, maman, je suis à la maison.   \n",
            "1           Et il a dit, maman, je suis à la maison.   \n",
            "2  Je ne savais pas dans quoi je me lançais, donc...   \n",
            "3  Je ne savais pas dans quoi je me lançais, donc...   \n",
            "4  Je ne savais pas dans quoi je me lançais, donc...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                             Il n'a pas dit un mot.  contradiction  \n",
            "1             Il a dit à sa mère qu'il était rentré.     entailment  \n",
            "2  Je ne suis jamais allé à Washington et donc qu...        neutral  \n",
            "3  Je savais exactement ce que j'avais à faire qu...  contradiction  \n",
            "4  Je n'étais pas tout à fait certain de ce que j...     entailment  \n",
            "***************************\n",
            "\n",
            "hi test set:\n",
            "                                             premise  \\\n",
            "0                  और उसने कहा, माँ, मैं घर आया हूं।   \n",
            "1                  और उसने कहा, माँ, मैं घर आया हूं।   \n",
            "2  मुझे नहीं पता था कि मैं क्या कर रहा था या कुछ ...   \n",
            "3  मुझे नहीं पता था कि मैं क्या कर रहा था या कुछ ...   \n",
            "4  मुझे नहीं पता था कि मैं क्या कर रहा था या कुछ ...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                             उसने एक शब्द नहीं कहा।  contradiction  \n",
            "1          Uski maa ne bataya ki wo ghar pahuch gaya     entailment  \n",
            "2  मैं कभी वाशिंगटन नहीं गया, इसलिए जब मुझे काम स...        neutral  \n",
            "3  मैं बिल्कुल जानता था कि मुझे वाशिंगटन जाने के ...  contradiction  \n",
            "4  Mujhe kuch nahi pata tha kya karna hai, to mai...     entailment  \n",
            "***************************\n",
            "\n",
            "ru test set:\n",
            "                                             premise  \\\n",
            "0                         И он сказал: Мама, я дома.   \n",
            "1                         И он сказал: Мама, я дома.   \n",
            "2  Я не знал, что мне предстояло сделать и все та...   \n",
            "3  Я не знал, что мне предстояло сделать и все та...   \n",
            "4  Я не знал, что мне предстояло сделать и все та...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                           Он не произнес ни слова.  contradiction  \n",
            "1          Он сказал матери, что уже добрался домой.     entailment  \n",
            "2  Я раньше не был в Вашингтоне, поэтому, получив...        neutral  \n",
            "3  Я точно знал, что мне нужно сделать, когда вхо...  contradiction  \n",
            "4  Я не знал, что мне делать, поэтому я отправилс...     entailment  \n",
            "***************************\n",
            "\n",
            "sw test set:\n",
            "                                             premise  \\\n",
            "0                 Naye akasema, Mama, niko nyumbani.   \n",
            "1                 Naye akasema, Mama, niko nyumbani.   \n",
            "2  Sikujua nini nilichoendea au kitu chochote, hi...   \n",
            "3  Sikujua nini nilichoendea au kitu chochote, hi...   \n",
            "4  Sikujua nini nilichoendea au kitu chochote, hi...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                                 Hakusema chochote.  contradiction  \n",
            "1     Alimwambia mama yake alikuwa amefika nyumbani.     entailment  \n",
            "2  Sijawahi kwenda Washington hivyo wakati nilipo...        neutral  \n",
            "3  Nilijua hasa kile nilichohitaji kufanya  nilip...  contradiction  \n",
            "4  Sikuwa na hakika kabisa nilichokuwa nikienda k...     entailment  \n",
            "***************************\n",
            "\n",
            "th test set:\n",
            "                                             premise  \\\n",
            "0                    และเขาพูดว่า, ม่าม๊า ผมอยู่บ้าน   \n",
            "1                    และเขาพูดว่า, ม่าม๊า ผมอยู่บ้าน   \n",
            "2  ฉันไม่รู้ว่าฉันไปเพื่ออะไรหรือเพื่อสิ่งใด ดังน...   \n",
            "3  ฉันไม่รู้ว่าฉันไปเพื่ออะไรหรือเพื่อสิ่งใด ดังน...   \n",
            "4  ฉันไม่รู้ว่าฉันไปเพื่ออะไรหรือเพื่อสิ่งใด ดังน...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                                  เขาไม่ได้พูดสักคำ  contradiction  \n",
            "1                    เขาบอกเเม่เขาว่าเขาถึงบ้านเเล้ว     entailment  \n",
            "2  ฉันไม่เคยไป กรุงวอชิงตันมาก่อนเลย เพราะเช่นนั้...        neutral  \n",
            "3  ฉันรู้อยู่เเล้วว่าฉันจะต้องทำยังไงจะปรับตัวกับ...  contradiction  \n",
            "4  ฉันไม่ค่อยมั่นใจว่าฉันจะทำอะไร ดังนั้นฉันจึงไป...     entailment  \n",
            "***************************\n",
            "\n",
            "tr test set:\n",
            "                                             premise  \\\n",
            "0                             Ve Anne, evdeyim dedi.   \n",
            "1                             Ve Anne, evdeyim dedi.   \n",
            "2  Ne için gittiğimi falan bilmiyordum, Washingto...   \n",
            "3  Ne için gittiğimi falan bilmiyordum, Washingto...   \n",
            "4  Ne için gittiğimi falan bilmiyordum, Washingto...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                              Bir kelime söylemedi.  contradiction  \n",
            "1                    Annesine eve gittiğini söyledi.     entailment  \n",
            "2  Washington'a hiç gitmedim, bu yüzden oraya ata...        neutral  \n",
            "3  Washington'a yürürken ne yapmam gerektiğini ta...  contradiction  \n",
            "4  Ne yapacağımdan çok emin değildim, o yüzden ra...     entailment  \n",
            "***************************\n",
            "\n",
            "ur test set:\n",
            "                                             premise  \\\n",
            "0              اور اس نے کہا امّی، میں گھر آگیا ہوں۔   \n",
            "1              اور اس نے کہا امّی، میں گھر آگیا ہوں۔   \n",
            "2  مجھے نہیں معلوم تھا کہ میں کیا کرنے جا رہا تھا...   \n",
            "3  مجھے نہیں معلوم تھا کہ میں کیا کرنے جا رہا تھا...   \n",
            "4  مجھے نہیں معلوم تھا کہ میں کیا کرنے جا رہا تھا...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                               وں ا یک لفز نھی بولا  contradiction  \n",
            "1               اسنی اپنی امی کو بتایا کے وں گھر ھیں     entailment  \n",
            "2  میں کبھی واشنگٹن نہیں گیا/گئی چناں چہ جب میرا ...        neutral  \n",
            "3  میں جانتا تھا کہ واشنگٹن کی طرف روانہ ہونے کے ...  contradiction  \n",
            "4  میں بالکل اس بات کا پتا نہی تھا کہ میں کیا کرن...     entailment  \n",
            "***************************\n",
            "\n",
            "vi test set:\n",
            "                                             premise  \\\n",
            "0                  Và anh ấy nói, Mẹ, con đã về nhà.   \n",
            "1                  Và anh ấy nói, Mẹ, con đã về nhà.   \n",
            "2  Tôi đã không biết mình đang hướng tới mục đích...   \n",
            "3  Tôi đã không biết mình đang hướng tới mục đích...   \n",
            "4  Tôi đã không biết mình đang hướng tới mục đích...   \n",
            "\n",
            "                                          hypothesis          label  \n",
            "0                         Anh không nói một lời nào.  contradiction  \n",
            "1                 Anh nói với mẹ rằng anh đã về nhà.     entailment  \n",
            "2  Tôi chưa bao giờ đến Washington nên khi tôi đư...        neutral  \n",
            "3  Tôi biết chính xác những gì tôi cần làm khi tô...  contradiction  \n",
            "4  Tôi đã không hoàn toàn chắc chắn những gì tôi ...     entailment  \n",
            "***************************\n",
            "\n",
            "zh test set:\n",
            "                           premise                          hypothesis  \\\n",
            "0                      他说，妈妈，我回来了。                             他没说一句话。   \n",
            "1                      他说，妈妈，我回来了。                     他告诉他的妈妈他已经回到家了。   \n",
            "2  我不知道我要去干什么还是什么的，所以就去华盛顿指定的地方报到。  我从来没有去过华盛顿，所以当我被派到那里时，为了找地方我都找迷路了。   \n",
            "3  我不知道我要去干什么还是什么的，所以就去华盛顿指定的地方报到。                  在我游行到华盛顿的时候我知道我要什么   \n",
            "4  我不知道我要去干什么还是什么的，所以就去华盛顿指定的地方报到。          我不确定我要做什么，所以我去了华盛顿。我被派去述职。   \n",
            "\n",
            "           label  \n",
            "0  contradiction  \n",
            "1     entailment  \n",
            "2        neutral  \n",
            "3  contradiction  \n",
            "4     entailment  \n",
            "***************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for lang, test_df in lang2test_df.items():\n",
        "    print(f\"{lang} test set:\")\n",
        "    print(test_df.head())\n",
        "    print(\"***************************\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc90924",
      "metadata": {
        "id": "4bc90924"
      },
      "source": [
        "## mBERT using HuggingFace's transformers library\n",
        "\n",
        "mBERT is a multilingual variant of BERT, which is trained on wikipedia articles in around [100 languages](BertTokenizer). Like monolingual BERT the transformers library also provides pre-trained models and tokenizers for multilingual BERT. To create an instance of one, we only need to specify `\"bert-base-multilingual-cased\"` or `\"bert-base-multilingual-uncased\"` in `BertTokenizer.from_pretrained` and `BertModel.from_pretrained` methods and that's it! See examples below for a demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "821b8b04",
      "metadata": {
        "id": "821b8b04"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bfddbd7c",
      "metadata": {
        "id": "bfddbd7c"
      },
      "outputs": [],
      "source": [
        "mbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "042ad78a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "042ad78a",
        "outputId": "da74a401-83be-4df2-a783-f88b8b39d6d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['thinking', 'machines']"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mbert_tokenizer.tokenize(\"thinking machines\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "deeed8c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "deeed8c1",
        "outputId": "1ab9fb82-5c37-401c-dcf0-0a720738e39e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['maquinas', 'de', 'pensar']"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mbert_tokenizer.tokenize(\"maquinas de pensar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3b7157dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3b7157dc",
        "outputId": "5db4f627-ceb5-48ba-99a7-999dcd36f403"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['स', '##ो', '##च', 'म', '##शी', '##न']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mbert_tokenizer.tokenize(\"सोच मशीन\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20fe02a",
      "metadata": {
        "id": "b20fe02a"
      },
      "source": [
        "As you can see mBERT's tokenizer works on different languages. We can similarly load a pretrained mbert model and feed data in different languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d403c05e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d403c05e",
        "outputId": "40cd00cd-ed82-4d81-b185-86bc739316f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "mbert_model = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b63671ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b63671ba",
        "outputId": "c15eacd6-01de-467c-e61f-38f7b469c850"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mbert_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96bce87e",
      "metadata": {
        "id": "96bce87e"
      },
      "source": [
        "As you can see the architecture is identical to the original BERT model. The only thing that is different is the shape of word_embeddings which is 105879 X 768, meaning there are 105879 unique tokens supported by mBERT (uncased). In contrast BERT (uncased) supports 30522 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "edeb75d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "edeb75d8",
        "outputId": "0106a094-d9aa-49bb-fcd0-05253a94576e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[-0.0295,  0.0307,  0.0245,  ...,  0.0328, -0.0562,  0.0789],\n",
              "                                                        [ 0.2827,  0.4737, -0.1378,  ..., -0.1892,  0.1408, -0.3370],\n",
              "                                                        [ 0.1871,  0.6193,  0.1692,  ...,  0.0987, -0.0519, -0.0380],\n",
              "                                                        [-0.0354,  0.4805, -0.2533,  ...,  0.7390,  0.1286, -0.6764]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[ 7.3179e-02,  1.0201e-01,  1.0094e-01,  9.1636e-02,  1.4111e-01,\n",
              "                                                         3.3841e-01,  9.0549e-02, -6.1101e-02, -1.3902e-01,  2.4607e-01,\n",
              "                                                        -1.7623e-01, -1.2388e-01,  1.7149e-01, -1.2340e-01, -1.6021e-01,\n",
              "                                                         2.9057e-02,  1.3472e-01,  7.6387e-02,  1.5949e-01,  4.8040e-02,\n",
              "                                                         3.7260e-02, -3.0617e-02,  5.3897e-02,  6.3597e-02,  2.1218e-01,\n",
              "                                                        -1.0945e-01,  2.1902e-01,  8.5052e-02,  2.2458e-01,  2.5002e-01,\n",
              "                                                         1.8784e-01,  1.9675e-01,  1.9028e-01, -6.7974e-02,  1.2689e-01,\n",
              "                                                        -5.0750e-02,  8.9531e-02,  7.3671e-02,  2.0277e-01,  4.1014e-02,\n",
              "                                                         1.2571e-01,  2.5094e-01,  9.7308e-02, -5.8275e-02, -3.0980e-01,\n",
              "                                                         9.3865e-02, -1.3890e-01, -6.5643e-02,  9.9998e-01,  1.3597e-01,\n",
              "                                                         7.1079e-02, -1.7904e-02,  4.0144e-02, -2.4498e-01,  2.1730e-01,\n",
              "                                                         9.9998e-01, -2.9374e-01, -2.0635e-01,  3.9283e-03, -1.2170e-01,\n",
              "                                                        -9.5930e-02,  5.2731e-02,  2.7847e-01,  6.3724e-02, -4.3806e-02,\n",
              "                                                         1.0382e-01, -4.0369e-02,  3.1189e-01,  5.5141e-02,  1.0137e-01,\n",
              "                                                         6.5947e-02, -5.4928e-02,  1.9506e-01,  2.7538e-01, -1.2576e-01,\n",
              "                                                        -3.4104e-02, -1.4140e-01, -9.4945e-02, -8.6547e-02,  1.5645e-01,\n",
              "                                                         1.0313e-01,  4.9405e-02, -2.0957e-01,  1.9060e-01,  2.2056e-02,\n",
              "                                                        -2.0124e-01, -1.5448e-01,  1.0537e-03,  9.0470e-02, -1.2185e-01,\n",
              "                                                         3.1289e-02, -6.4975e-02, -7.1926e-02, -9.2560e-02,  1.5431e-01,\n",
              "                                                        -1.1182e-01, -2.0971e-01,  2.2940e-02,  6.8358e-02, -2.0433e-01,\n",
              "                                                        -2.1997e-02,  1.1978e-01, -4.2444e-03,  1.4974e-01,  1.5300e-01,\n",
              "                                                         6.4774e-02, -1.8277e-01, -1.0238e-01,  6.4092e-02, -2.1379e-02,\n",
              "                                                        -2.0232e-01, -4.0598e-02,  3.0287e-01,  1.1264e-01,  4.2799e-02,\n",
              "                                                        -4.9957e-02, -1.6272e-01, -5.2993e-01,  2.4338e-02,  8.4274e-02,\n",
              "                                                        -7.2513e-02,  9.9998e-01, -8.1503e-02, -1.0315e-01,  1.0807e-01,\n",
              "                                                        -1.6528e-01, -7.4172e-02,  1.7210e-01, -1.7617e-01,  2.3838e-01,\n",
              "                                                        -2.4627e-01, -3.5075e-02, -2.0167e-01,  1.1083e-02, -2.1585e-01,\n",
              "                                                         1.9591e-01,  2.8572e-02, -1.3423e-01, -2.7209e-02, -1.9213e-01,\n",
              "                                                         4.5604e-02,  9.3544e-02,  9.3215e-02,  5.9238e-03,  1.4489e-01,\n",
              "                                                        -1.2675e-01, -8.2117e-02, -2.3945e-02,  5.8369e-02,  2.0040e-01,\n",
              "                                                        -2.7510e-01, -1.5230e-01, -6.6988e-02, -1.4324e-01,  4.8355e-02,\n",
              "                                                        -2.4482e-01,  6.4942e-02, -2.7865e-01,  1.9275e-01, -1.6717e-02,\n",
              "                                                        -2.0453e-01,  2.0115e-01,  2.0163e-01,  5.7708e-02,  1.0814e-01,\n",
              "                                                        -1.4691e-01,  5.0600e-01, -1.4347e-01,  1.4694e-01, -1.6047e-01,\n",
              "                                                         7.4324e-02,  5.1643e-02,  2.3387e-01,  1.5526e-01, -1.1582e-01,\n",
              "                                                        -2.3384e-01,  1.6088e-01, -1.5283e-02,  6.3142e-02, -2.9525e-01,\n",
              "                                                         1.7536e-02, -2.9049e-01, -8.7841e-02,  8.7400e-02,  7.3242e-02,\n",
              "                                                         4.4863e-03, -1.3267e-01, -4.5216e-01, -1.2429e-01, -1.5805e-01,\n",
              "                                                         1.1848e-01,  1.0525e-01,  8.2202e-02,  1.3625e-01,  1.3559e-01,\n",
              "                                                         9.6080e-02, -1.5444e-01, -1.3422e-01, -5.1552e-02,  3.8195e-02,\n",
              "                                                        -3.3402e-01, -2.0950e-02,  5.8303e-02, -1.1204e-01, -2.4186e-01,\n",
              "                                                         2.0704e-01, -1.0256e-01,  9.9998e-01,  1.1866e-01, -7.6897e-02,\n",
              "                                                         1.5445e-01, -4.3846e-02,  2.0678e-02,  1.4141e-02, -1.3867e-01,\n",
              "                                                         1.3031e-01,  1.3122e-01, -3.6650e-03,  1.0144e-01, -1.4299e-01,\n",
              "                                                        -1.6708e-01,  4.9734e-01,  2.1075e-01, -1.8210e-01, -1.5134e-02,\n",
              "                                                        -2.5535e-01, -5.7146e-02, -2.7071e-01,  8.0420e-02, -1.8984e-02,\n",
              "                                                         4.7518e-02,  1.2303e-01,  3.8837e-01, -2.0640e-01,  9.6626e-02,\n",
              "                                                        -2.8383e-01, -1.5654e-01, -1.9583e-01, -2.4416e-01, -1.3667e-01,\n",
              "                                                         6.2990e-02,  1.6517e-01, -9.4205e-02, -7.0088e-02,  2.2586e-01,\n",
              "                                                        -1.5455e-01,  6.8988e-02, -1.5306e-01, -1.4796e-01, -9.1485e-02,\n",
              "                                                         1.8129e-02, -8.6493e-03, -4.7684e-02,  2.5947e-01,  1.6200e-01,\n",
              "                                                         4.8667e-02, -7.7197e-02, -1.5778e-01,  2.4799e-02,  1.3296e-01,\n",
              "                                                         6.9303e-02,  9.7160e-03, -1.6599e-01, -1.1040e-01,  4.5165e-02,\n",
              "                                                        -1.8001e-01, -2.2770e-03,  1.8889e-02,  4.2017e-03,  7.3417e-02,\n",
              "                                                         1.2938e-01,  1.2569e-01, -5.7200e-02,  2.9756e-01, -3.4206e-01,\n",
              "                                                         9.0946e-02,  9.2744e-02, -2.3592e-01,  2.0424e-01,  1.3065e-01,\n",
              "                                                         5.1349e-02, -7.4663e-02,  5.8808e-03, -1.3135e-01,  2.5051e-02,\n",
              "                                                         1.7354e-01,  8.9000e-02, -1.2420e-01,  1.5896e-01,  2.4489e-01,\n",
              "                                                         6.5984e-02, -1.1487e-01, -7.8051e-02,  5.0933e-02,  1.4547e-01,\n",
              "                                                        -1.7255e-01,  2.7150e-01,  9.4294e-02, -1.1215e-01, -1.3279e-01,\n",
              "                                                         9.2363e-02, -1.3826e-01, -9.9183e-02,  1.7941e-02,  1.6031e-01,\n",
              "                                                         2.1074e-01, -1.6457e-01, -5.5598e-02, -1.2069e-01,  1.0693e-02,\n",
              "                                                         7.2653e-02, -6.5962e-02, -1.9477e-01,  2.1003e-01, -3.0987e-01,\n",
              "                                                         9.4053e-02,  1.9550e-01, -2.8578e-01, -7.0935e-03,  1.2259e-02,\n",
              "                                                         1.1221e-01,  9.9998e-01, -1.5399e-01, -1.5067e-01, -9.9998e-01,\n",
              "                                                        -4.0517e-01, -2.2675e-01,  2.4685e-01,  1.2637e-01, -2.0596e-01,\n",
              "                                                         9.4710e-03,  9.8535e-02, -1.5658e-01, -1.5616e-01,  1.1422e-01,\n",
              "                                                         3.4512e-01, -1.1153e-01,  9.7742e-02,  7.4420e-02,  1.6055e-02,\n",
              "                                                         9.3673e-02, -9.9997e-01,  1.3721e-02,  1.0505e-01,  4.5990e-02,\n",
              "                                                        -2.7098e-02, -1.7936e-01, -1.9624e-01, -7.5425e-02,  1.3567e-01,\n",
              "                                                         1.2713e-01,  1.2091e-01,  2.1556e-02, -1.5238e-01, -3.4905e-01,\n",
              "                                                         1.1866e-01, -5.9070e-02,  3.0714e-02, -1.7615e-01, -1.8791e-01,\n",
              "                                                         4.0728e-02,  1.5794e-01,  4.1043e-03,  2.5339e-01, -9.1826e-02,\n",
              "                                                         9.0279e-02, -1.4581e-01,  9.9998e-01,  1.3606e-01, -9.2719e-02,\n",
              "                                                         9.9998e-01,  2.4808e-01,  2.8235e-02, -3.3395e-01,  9.1213e-02,\n",
              "                                                         6.6393e-02,  9.9998e-01,  6.3202e-02, -1.9145e-01,  1.9956e-02,\n",
              "                                                        -8.7200e-02,  1.6400e-01,  3.1808e-01, -5.4950e-02, -9.9998e-01,\n",
              "                                                         3.9525e-02,  1.3308e-01,  8.9567e-02, -2.0165e-02,  9.9998e-01,\n",
              "                                                        -4.1042e-02, -9.1357e-02,  7.8819e-03,  2.8616e-01,  7.6935e-02,\n",
              "                                                         3.1532e-01,  1.8350e-01,  2.7013e-02, -1.6635e-01, -6.1900e-02,\n",
              "                                                        -7.2003e-02, -2.7173e-01, -1.1778e-01,  5.0166e-02, -6.7674e-02,\n",
              "                                                        -2.2937e-02, -1.5914e-01, -6.9601e-02,  2.1845e-02,  9.9998e-01,\n",
              "                                                        -5.6357e-02,  1.2465e-01,  5.5937e-02,  3.0309e-02,  8.3948e-02,\n",
              "                                                        -3.2562e-02,  9.1416e-02, -2.0855e-01,  2.4348e-01, -2.0648e-01,\n",
              "                                                         1.2498e-01, -2.9969e-02, -2.4256e-03, -1.8928e-01, -5.9992e-02,\n",
              "                                                        -1.8654e-01,  1.1466e-01,  3.2974e-02,  2.7450e-01,  9.8092e-02,\n",
              "                                                        -5.1888e-03, -4.1001e-02,  1.3183e-01,  3.1007e-04,  2.5039e-01,\n",
              "                                                        -1.2271e-01,  9.9998e-01,  1.4857e-01, -1.0520e-01,  4.4375e-02,\n",
              "                                                        -8.9653e-02, -1.6761e-01, -1.0846e-01, -2.6062e-01,  3.6854e-01,\n",
              "                                                         7.4351e-02,  1.4631e-01,  6.7138e-02,  2.0981e-01,  4.1744e-03,\n",
              "                                                        -8.7262e-02,  1.0625e-01,  4.5029e-02, -4.9726e-02, -1.6090e-01,\n",
              "                                                        -9.9999e-01, -2.4006e-01,  1.2973e-01, -1.7996e-01, -1.4771e-01,\n",
              "                                                         9.7282e-02, -2.4893e-01, -2.8951e-02, -1.0087e-01, -2.5671e-02,\n",
              "                                                         5.3566e-03, -1.7670e-01, -1.5131e-01,  7.0804e-02, -1.4243e-01,\n",
              "                                                        -1.8981e-01,  1.7267e-01,  1.5724e-01,  2.5108e-01,  3.8738e-02,\n",
              "                                                         2.4062e-01,  1.2826e-01, -1.4060e-02,  1.3581e-01, -2.5359e-02,\n",
              "                                                         2.8074e-03, -2.6812e-01, -2.8872e-01, -1.3352e-01, -1.5422e-01,\n",
              "                                                         1.3957e-01, -1.9933e-01,  3.8098e-02,  5.4995e-02, -1.1654e-01,\n",
              "                                                         2.3261e-01,  1.3446e-01,  4.0561e-02, -1.5145e-01,  2.4640e-02,\n",
              "                                                        -4.3278e-03,  6.3809e-02, -2.7966e-01,  1.8674e-01, -9.3060e-02,\n",
              "                                                         1.3904e-01, -3.1103e-02, -1.2904e-01,  1.3963e-01, -1.9823e-01,\n",
              "                                                         1.3823e-01,  1.3178e-01,  1.4865e-01, -4.3583e-02, -1.7827e-01,\n",
              "                                                        -2.8847e-02, -7.4495e-03,  2.0755e-01,  7.0420e-02, -8.1432e-02,\n",
              "                                                        -1.1339e-01, -7.5352e-02, -2.0457e-01,  3.9577e-02, -2.0638e-02,\n",
              "                                                        -9.9998e-01,  2.6041e-03, -1.4317e-01, -1.2968e-01,  1.6345e-01,\n",
              "                                                         1.3522e-02,  2.9230e-01, -7.4525e-02, -2.8082e-01,  2.1194e-01,\n",
              "                                                         4.2106e-03, -2.3653e-01, -5.0939e-02,  7.8465e-02,  8.5712e-03,\n",
              "                                                         4.9473e-02, -9.9998e-01, -2.4342e-02,  6.6554e-02, -1.9232e-01,\n",
              "                                                         1.0921e-01,  1.4569e-01, -3.5886e-02,  7.3304e-02, -6.5317e-02,\n",
              "                                                         6.8486e-01,  6.1876e-02,  8.5777e-02, -8.7385e-02, -8.7729e-02,\n",
              "                                                        -1.3302e-01, -1.5859e-01, -1.5562e-01,  1.6459e-01,  3.1978e-03,\n",
              "                                                         1.2600e-01, -1.6958e-01,  2.5404e-01,  4.9495e-02,  3.7814e-01,\n",
              "                                                         1.5938e-01, -7.9063e-02,  5.5768e-02, -5.4164e-02,  1.7684e-01,\n",
              "                                                         2.8788e-02,  2.4044e-01, -1.7441e-01,  7.5438e-02,  3.1538e-02,\n",
              "                                                        -1.3379e-01,  1.4529e-01,  1.6321e-01,  1.8002e-02,  2.7364e-01,\n",
              "                                                         1.0350e-01,  1.6075e-01, -6.0345e-02, -6.6107e-02,  1.5257e-02,\n",
              "                                                         5.3042e-02, -2.2797e-01,  2.5812e-02,  5.0953e-02,  6.9829e-02,\n",
              "                                                        -1.4106e-01,  1.0170e-01, -1.7497e-01,  6.7592e-02, -2.2314e-01,\n",
              "                                                        -1.6602e-01, -1.5866e-01,  5.3789e-02, -5.5261e-02, -1.7660e-01,\n",
              "                                                         1.8860e-01, -1.3061e-01, -2.0776e-01, -1.0591e-01,  4.6543e-02,\n",
              "                                                         9.5898e-02, -7.5416e-02, -2.9110e-02,  1.4028e-01,  2.0851e-01,\n",
              "                                                        -2.7833e-02,  1.4745e-01,  9.1486e-02, -5.3949e-02,  6.6658e-02,\n",
              "                                                         2.0588e-01, -1.2952e-01, -2.6022e-01,  1.8500e-01,  8.4620e-02,\n",
              "                                                         1.4916e-02,  1.7777e-01, -3.8061e-02, -2.3495e-01,  1.3902e-01,\n",
              "                                                         3.2566e-02, -4.0397e-02, -3.6005e-02,  1.2015e-01, -1.0909e-01,\n",
              "                                                        -8.8604e-02,  1.1901e-01, -9.9998e-01,  1.0674e-01, -8.9303e-02,\n",
              "                                                        -4.3718e-02,  2.6641e-01,  2.2693e-01,  1.5056e-01,  1.4210e-01,\n",
              "                                                         1.6559e-02, -2.3216e-01, -2.5339e-01, -1.7110e-02, -9.0951e-02,\n",
              "                                                        -3.2911e-01,  7.1875e-02, -1.3863e-01, -7.9049e-02,  1.5858e-01,\n",
              "                                                        -2.7387e-01, -1.9463e-01, -9.5768e-02, -6.6397e-02,  1.5667e-01,\n",
              "                                                         8.5604e-02,  3.7492e-03, -1.7165e-01, -6.4464e-02,  4.7901e-02,\n",
              "                                                        -2.8880e-01, -2.5597e-01,  2.0448e-01,  2.2661e-01,  1.8551e-01,\n",
              "                                                        -4.0198e-01,  1.8714e-01, -1.2346e-01, -6.3954e-02,  5.8401e-03,\n",
              "                                                         1.6813e-01,  7.9842e-04, -1.1464e-01,  2.6567e-01, -1.1165e-01,\n",
              "                                                        -2.1061e-01,  6.7415e-02, -2.5445e-01,  1.1992e-01, -6.3134e-01,\n",
              "                                                         1.9971e-01,  9.3713e-02,  3.0368e-01, -1.5028e-01,  2.2328e-01,\n",
              "                                                         8.5728e-02,  4.6135e-02,  5.7843e-03,  1.4339e-01, -7.3669e-03,\n",
              "                                                         2.8754e-01, -3.1202e-02,  2.2612e-01,  3.4620e-01, -9.9998e-01,\n",
              "                                                        -3.6616e-02, -5.4562e-02, -1.5097e-01, -9.9998e-01, -8.8908e-02,\n",
              "                                                         1.2939e-01,  5.8688e-02,  1.5712e-01,  5.9310e-02,  1.4333e-01,\n",
              "                                                         1.9591e-01, -1.4970e-01, -1.1364e-01,  1.6807e-01, -1.9834e-01,\n",
              "                                                        -5.9953e-02,  1.9273e-01, -1.3947e-01, -1.7666e-01,  1.1213e-01,\n",
              "                                                         1.8712e-01,  1.6260e-01,  7.3317e-02, -2.6980e-01,  4.3758e-01,\n",
              "                                                        -8.4492e-02, -4.7005e-02, -3.3171e-01, -1.0506e-01,  2.1018e-01,\n",
              "                                                        -6.6405e-02,  5.1141e-02,  1.3922e-01,  2.2156e-01, -8.6738e-02,\n",
              "                                                        -1.8886e-01, -5.7752e-02, -8.9208e-02,  2.6356e-01,  9.5995e-02,\n",
              "                                                         1.1035e-01, -2.0700e-02,  2.8573e-01, -1.5227e-01, -6.7774e-02,\n",
              "                                                        -1.2150e-03, -3.9227e-03,  2.1186e-01, -1.6579e-01, -1.0625e-01,\n",
              "                                                         3.1726e-01, -1.9441e-01, -8.0780e-02,  1.5680e-01,  1.2033e-01,\n",
              "                                                        -1.8308e-01, -6.0178e-02, -9.5076e-02,  2.9383e-01, -9.9998e-01,\n",
              "                                                        -1.3483e-02, -5.7824e-02,  1.3638e-01,  1.1943e-02, -9.4663e-02,\n",
              "                                                         6.3522e-01,  2.0999e-01, -8.0715e-03,  2.4701e-01, -2.1165e-01,\n",
              "                                                        -8.7867e-02, -1.8082e-01,  1.1048e-01,  4.5320e-02, -9.6414e-03,\n",
              "                                                         1.1225e-02, -1.1681e-01, -1.1469e-01]], grad_fn=<TanhBackward0>))])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_sent = \"thinking machines\"\n",
        "tokenizer_output = mbert_tokenizer(en_sent, return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "\n",
        "mbert_model(input_ids, attention_mask = attn_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "829a3a77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "829a3a77",
        "outputId": "342833b2-59a8-4e1e-c271-de677813584d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[-0.0657, -0.0614,  0.0060,  ..., -0.0240, -0.0420, -0.0673],\n",
              "                                                        [ 0.2430,  0.5252, -0.0694,  ..., -0.0108,  0.5518, -0.4204],\n",
              "                                                        [-0.2587, -0.2086,  0.1115,  ..., -0.7734,  0.1412, -0.9133],\n",
              "                                                        [-0.1032,  0.7808,  0.1022,  ...,  0.0870,  0.1456, -0.5586],\n",
              "                                                        [-0.0765,  0.3234, -0.2513,  ...,  0.3950,  0.2862, -1.1907]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[ 1.4176e-01,  3.4401e-02,  1.5948e-01,  1.7631e-01,  1.8552e-01,\n",
              "                                                         4.2113e-01,  1.4076e-01, -1.4554e-01, -1.7636e-01,  2.9590e-01,\n",
              "                                                        -1.8210e-01, -2.1683e-01,  2.2756e-01, -1.7093e-01, -1.8114e-01,\n",
              "                                                         5.4472e-02,  1.9600e-01,  1.2176e-01,  2.0155e-01,  1.3893e-01,\n",
              "                                                        -4.2202e-02, -8.1407e-02,  8.8412e-02, -1.3793e-02,  2.7080e-01,\n",
              "                                                        -1.6806e-01,  2.6696e-01,  1.2918e-01,  3.6292e-01,  2.9422e-01,\n",
              "                                                         2.0838e-01,  2.5432e-01,  2.5948e-01, -1.1543e-01,  2.1006e-01,\n",
              "                                                         2.0638e-02,  3.6320e-02,  1.2488e-01,  2.3027e-01,  8.2570e-02,\n",
              "                                                         1.4708e-01,  4.4298e-01,  1.5398e-01, -1.5360e-01, -3.4018e-01,\n",
              "                                                         2.2819e-01, -1.9345e-01, -8.5304e-02,  9.9999e-01,  1.9508e-01,\n",
              "                                                         8.9446e-02, -1.4110e-01,  1.0632e-01, -2.7657e-01,  2.4497e-01,\n",
              "                                                         9.9999e-01, -3.4471e-01, -2.3381e-01,  5.2617e-02, -1.3738e-01,\n",
              "                                                        -2.0170e-01,  7.5959e-02,  3.1516e-01,  1.3019e-01, -1.1624e-01,\n",
              "                                                         1.2369e-01, -6.2276e-02,  3.3121e-01,  1.2949e-02,  1.1194e-01,\n",
              "                                                         1.0270e-01, -1.0232e-01,  2.2664e-01,  3.0402e-01, -1.8597e-01,\n",
              "                                                         4.0614e-02, -2.0928e-01, -2.0389e-01, -1.9426e-01,  1.9920e-01,\n",
              "                                                         1.9141e-01,  8.9631e-02, -2.5999e-01,  2.3049e-01, -9.8978e-02,\n",
              "                                                        -2.9516e-01, -2.1329e-01, -7.9474e-02,  1.3188e-01, -2.2406e-01,\n",
              "                                                         9.4448e-02, -1.0134e-01, -9.9620e-02, -1.3253e-01,  1.6258e-01,\n",
              "                                                        -1.6331e-01, -2.5396e-01, -7.3900e-02,  1.3007e-01, -2.8521e-01,\n",
              "                                                        -5.7481e-02,  1.6074e-01,  4.0952e-02,  1.1781e-01,  2.0784e-01,\n",
              "                                                         2.3460e-01, -2.5248e-01, -1.3074e-01, -1.0675e-02, -9.2083e-03,\n",
              "                                                        -2.2128e-01,  4.4169e-02,  4.5098e-01,  2.1983e-01,  8.7659e-02,\n",
              "                                                         2.7385e-02, -1.8454e-01, -7.4264e-01, -5.1756e-02,  1.4141e-01,\n",
              "                                                        -3.4279e-02,  9.9999e-01, -1.6274e-01, -1.8138e-01,  1.5650e-01,\n",
              "                                                        -2.6184e-01, -8.8531e-02,  2.2055e-01, -2.1966e-01,  3.1545e-01,\n",
              "                                                        -2.9460e-01, -8.4367e-02, -2.5458e-01,  4.0762e-02, -2.7454e-01,\n",
              "                                                         2.8245e-01,  8.0520e-02, -1.1288e-01,  5.4163e-02, -2.4504e-01,\n",
              "                                                         7.4325e-02,  1.4228e-01,  1.4912e-01,  1.3070e-01,  1.9802e-01,\n",
              "                                                        -1.9757e-01, -9.4446e-02,  1.2255e-02, -4.4503e-02,  2.2847e-01,\n",
              "                                                        -3.0940e-01, -2.4367e-01, -3.6847e-02, -1.9609e-01,  4.9528e-03,\n",
              "                                                        -3.1056e-01,  1.1333e-01, -3.2450e-01,  2.2749e-01,  7.3080e-02,\n",
              "                                                        -2.3836e-01,  2.3000e-01,  2.5693e-01,  1.3141e-01,  1.7909e-01,\n",
              "                                                        -1.7676e-01,  7.1778e-01, -1.9316e-01,  2.4662e-01, -2.3022e-01,\n",
              "                                                        -7.1705e-02,  1.1786e-01,  2.7383e-01,  1.9487e-01, -1.4186e-01,\n",
              "                                                        -2.8495e-01,  2.0092e-01, -7.7067e-02,  1.3877e-01, -4.7296e-01,\n",
              "                                                         7.2584e-02, -3.7574e-01, -1.2160e-01,  1.4101e-01,  3.9108e-02,\n",
              "                                                         4.1496e-02, -2.3796e-01, -6.5369e-01, -4.6147e-02, -2.4818e-01,\n",
              "                                                         1.7878e-01,  1.8163e-01,  1.3553e-01,  2.3442e-01,  1.8634e-01,\n",
              "                                                        -1.0071e-02, -1.9418e-01, -2.2230e-01, -7.7779e-02,  1.1157e-01,\n",
              "                                                        -4.3845e-01,  4.1401e-02, -5.1808e-02, -1.6167e-01, -2.8458e-01,\n",
              "                                                         2.7338e-01, -1.7314e-01,  9.9999e-01,  6.0498e-02, -1.6026e-01,\n",
              "                                                         1.6814e-01, -1.4584e-01, -5.7390e-02, -5.6743e-02, -2.1679e-01,\n",
              "                                                         1.5827e-01,  1.6530e-01, -8.8480e-02,  1.8519e-01, -2.3680e-01,\n",
              "                                                        -2.3501e-01,  6.7256e-01,  2.8943e-01, -2.2693e-01, -6.8995e-02,\n",
              "                                                        -2.8145e-01, -1.9822e-02, -3.4394e-01,  2.2847e-01, -1.2438e-01,\n",
              "                                                         8.7177e-02,  1.5882e-01,  4.0037e-01, -2.5549e-01,  1.4033e-01,\n",
              "                                                        -3.3692e-01, -2.1416e-01, -2.2396e-01, -4.0181e-01, -2.0030e-01,\n",
              "                                                         1.0835e-01,  1.6841e-01, -1.4286e-01, -5.5853e-02,  2.6443e-01,\n",
              "                                                        -2.0406e-01,  1.3307e-01, -2.1589e-01, -2.5919e-01, -1.5416e-01,\n",
              "                                                         4.1412e-02, -1.3100e-01, -1.1098e-01,  2.7552e-01,  1.8748e-01,\n",
              "                                                         1.2460e-01, -1.0504e-01, -1.6342e-01,  8.9278e-02,  1.9794e-01,\n",
              "                                                         1.5078e-01,  3.2019e-02, -2.2913e-01, -1.0459e-01, -1.0096e-02,\n",
              "                                                        -2.5375e-01,  6.5590e-02,  6.1361e-02, -1.2471e-01,  1.0422e-01,\n",
              "                                                         3.1563e-01,  2.0883e-01, -1.0141e-01,  3.4946e-01, -4.2757e-01,\n",
              "                                                         1.1210e-01,  1.7992e-01, -2.4741e-01,  2.5516e-01,  1.5931e-01,\n",
              "                                                        -1.8456e-02, -1.2316e-01,  7.8277e-02, -1.6251e-01,  7.2417e-02,\n",
              "                                                         2.2503e-01,  1.5575e-01, -1.6483e-01,  1.4196e-01,  3.0710e-01,\n",
              "                                                         1.3345e-01, -1.7176e-01, -1.1518e-01,  1.4071e-01,  2.1860e-01,\n",
              "                                                        -2.0527e-01,  3.1394e-01,  1.8230e-01, -1.8522e-01, -1.5367e-01,\n",
              "                                                         1.1750e-01, -1.9448e-01, -1.7304e-01, -5.1064e-02,  2.6404e-01,\n",
              "                                                         2.3060e-01, -2.1061e-01, -1.2781e-01, -2.0407e-01, -4.5289e-02,\n",
              "                                                         8.3658e-02, -1.5178e-01, -2.6070e-01,  2.2416e-01, -3.5798e-01,\n",
              "                                                         1.1783e-01,  2.2576e-01, -3.2027e-01, -5.1004e-02, -5.5694e-02,\n",
              "                                                         1.8284e-01,  9.9999e-01, -2.1127e-01, -1.3878e-01, -9.9999e-01,\n",
              "                                                        -4.6576e-01, -2.9456e-01,  2.7522e-01,  1.6385e-01, -2.7458e-01,\n",
              "                                                        -7.8472e-02,  1.3114e-01, -1.9779e-01, -2.3622e-01,  2.1868e-01,\n",
              "                                                         3.8457e-01, -1.8298e-01,  1.2252e-01,  1.0747e-01,  2.9732e-02,\n",
              "                                                         1.2897e-01, -9.9998e-01, -4.7202e-02,  1.3700e-02, -3.2895e-02,\n",
              "                                                        -7.5726e-02, -2.2144e-01, -2.1903e-01, -1.2936e-01,  1.8736e-01,\n",
              "                                                         1.9446e-01,  1.7180e-01, -7.0668e-02, -2.5061e-01, -3.4393e-01,\n",
              "                                                         1.5563e-01, -1.3025e-01,  6.0536e-02, -2.0935e-01, -2.5558e-01,\n",
              "                                                         1.0602e-01,  2.3848e-01,  1.9070e-02,  2.9148e-01, -1.4300e-01,\n",
              "                                                         1.8768e-01, -2.1562e-01,  9.9999e-01,  2.1174e-01, -1.7241e-01,\n",
              "                                                         9.9999e-01,  2.9368e-01, -2.0803e-02, -4.4702e-01,  1.0769e-01,\n",
              "                                                         1.0370e-01,  9.9999e-01,  1.5182e-01, -2.5394e-01, -8.5451e-02,\n",
              "                                                         4.6755e-02,  2.4464e-01,  3.8935e-01, -5.4863e-03, -9.9999e-01,\n",
              "                                                         1.0994e-01,  1.7869e-01,  3.3433e-03, -4.7390e-02,  9.9999e-01,\n",
              "                                                        -9.2318e-02, -1.8041e-01,  7.8401e-02,  3.1596e-01,  1.4380e-01,\n",
              "                                                         3.7415e-01,  1.8647e-01, -2.8230e-02, -2.3531e-01, -9.2129e-02,\n",
              "                                                        -1.3040e-01, -3.2820e-01, -1.7585e-01,  1.0518e-01, -1.1582e-01,\n",
              "                                                        -1.0651e-01, -2.4714e-01,  1.1307e-02, -5.5399e-02,  9.9999e-01,\n",
              "                                                        -2.4822e-02,  1.9087e-01,  6.9512e-02,  1.3101e-01,  1.0372e-01,\n",
              "                                                        -7.8402e-02, -1.4170e-02, -2.8408e-01,  2.8636e-01, -2.6532e-01,\n",
              "                                                         1.8592e-01,  7.6087e-02,  5.1574e-02, -2.3418e-01, -1.0005e-01,\n",
              "                                                        -2.0759e-01,  1.8586e-01,  6.1952e-02,  4.4869e-01,  1.5651e-01,\n",
              "                                                        -9.8914e-02, -1.2176e-01,  1.8979e-01, -2.7777e-02,  2.8522e-01,\n",
              "                                                        -1.6327e-01,  9.9999e-01,  2.0271e-01, -1.7230e-01, -9.5023e-03,\n",
              "                                                        -1.1205e-01, -2.2409e-01, -1.8732e-01, -2.7898e-01,  5.4942e-01,\n",
              "                                                         1.3951e-01,  9.7582e-02,  1.6390e-01,  2.6419e-01, -7.2977e-02,\n",
              "                                                        -9.9677e-02,  1.4631e-01,  3.5513e-02,  7.0371e-03, -2.9917e-01,\n",
              "                                                        -9.9999e-01, -2.7512e-01,  1.8115e-01, -3.4174e-01, -2.1146e-01,\n",
              "                                                         3.7808e-02, -3.2011e-01, -8.5779e-02, -3.9099e-02,  8.2518e-03,\n",
              "                                                         8.7735e-02, -2.5740e-01, -2.0243e-01,  1.1256e-01, -1.3412e-01,\n",
              "                                                        -2.1730e-01,  1.9210e-01,  2.7734e-01,  2.7189e-01, -9.4020e-02,\n",
              "                                                         2.7952e-01,  1.8280e-01,  1.1657e-02,  2.1934e-01, -7.1974e-02,\n",
              "                                                         6.1104e-02, -3.5227e-01, -3.3142e-01, -1.9634e-01, -1.9735e-01,\n",
              "                                                         2.0644e-01, -2.4785e-01, -3.3867e-02,  1.2497e-01, -1.9365e-01,\n",
              "                                                         2.6553e-01,  1.8153e-01,  8.9658e-03, -1.6964e-01,  8.8955e-02,\n",
              "                                                        -6.2141e-02,  1.4524e-01, -3.3219e-01,  2.3616e-01, -1.8622e-02,\n",
              "                                                         2.2127e-01, -1.5129e-01, -6.2764e-02,  1.9729e-01, -2.5878e-01,\n",
              "                                                         1.7096e-01,  1.6829e-01,  1.9079e-01, -1.1256e-01, -2.2692e-01,\n",
              "                                                         9.4886e-04,  3.8270e-02,  2.4418e-01,  1.2216e-01, -1.7033e-01,\n",
              "                                                        -1.5731e-01, -1.6348e-01, -2.3694e-01, -3.7750e-04,  5.7342e-02,\n",
              "                                                        -9.9999e-01, -6.2483e-02, -1.4103e-01, -1.9935e-01,  2.1452e-01,\n",
              "                                                         1.0831e-01,  3.4353e-01, -6.4320e-02, -4.8609e-01,  2.9674e-01,\n",
              "                                                        -7.2625e-02, -2.7103e-01, -4.3679e-02,  1.3353e-01, -5.0690e-02,\n",
              "                                                         5.9567e-02, -9.9999e-01,  4.1516e-02,  1.8530e-01, -2.1969e-01,\n",
              "                                                         1.4529e-01,  1.9153e-01, -1.0433e-01,  1.0763e-01, -8.1715e-02,\n",
              "                                                         8.5897e-01,  8.7412e-02,  1.0658e-01, -1.1136e-01, -3.6795e-03,\n",
              "                                                        -1.7956e-01, -2.8567e-01, -1.8426e-01,  1.7989e-01, -8.2679e-03,\n",
              "                                                         1.4666e-01, -2.3866e-01,  2.7470e-01,  8.0881e-02,  4.1714e-01,\n",
              "                                                         1.9624e-01, -7.9879e-02,  6.6923e-02, -1.3577e-01,  2.4110e-01,\n",
              "                                                         5.0170e-02,  2.7440e-01, -1.0605e-01,  1.7907e-02,  1.4192e-01,\n",
              "                                                        -1.9542e-01,  1.7607e-01,  1.6831e-01,  8.5068e-02,  3.2943e-01,\n",
              "                                                         2.0415e-01,  2.2167e-01, -1.0445e-01, -1.1645e-01,  1.0808e-01,\n",
              "                                                         1.1252e-01, -2.6779e-01,  5.5929e-02,  1.6692e-01,  3.4567e-02,\n",
              "                                                        -6.8955e-02,  9.7206e-02, -2.1742e-01,  1.2839e-01, -2.3346e-01,\n",
              "                                                        -2.0766e-01, -2.3677e-01,  1.1460e-01, -1.0576e-01, -2.5598e-01,\n",
              "                                                         2.5922e-01, -1.7811e-01, -2.4270e-01, -1.0195e-01, -4.3534e-02,\n",
              "                                                         2.1582e-01, -1.4364e-01, -7.8588e-02,  1.8210e-01,  2.7148e-01,\n",
              "                                                         6.4059e-02,  2.2729e-01,  1.4698e-01, -1.0750e-01,  5.3824e-02,\n",
              "                                                         1.9585e-01, -2.0472e-01, -3.1686e-01,  2.0575e-01,  1.4279e-01,\n",
              "                                                         7.3550e-02,  1.9425e-01,  1.9031e-02, -2.7806e-01,  1.7672e-01,\n",
              "                                                         8.6702e-02, -5.5433e-02, -1.4306e-01,  1.4652e-01, -1.5505e-01,\n",
              "                                                        -9.4883e-02,  1.9275e-01, -9.9999e-01,  7.2696e-02, -1.1300e-01,\n",
              "                                                        -1.3813e-01,  3.3507e-01,  2.6896e-01,  1.7496e-01,  2.0019e-01,\n",
              "                                                        -8.8430e-03, -2.8412e-01, -2.7504e-01, -6.9042e-02, -5.6403e-02,\n",
              "                                                        -5.0014e-01,  1.2411e-01, -1.8666e-02, -8.8522e-02,  1.5650e-01,\n",
              "                                                        -4.0721e-01, -2.3049e-01, -1.5754e-01, -8.8076e-02,  2.1587e-01,\n",
              "                                                         1.3142e-01,  6.6759e-02, -3.6864e-01, -1.3832e-01, -1.4163e-02,\n",
              "                                                        -3.2093e-01, -2.9088e-01,  2.3202e-01,  2.5552e-01,  2.6093e-01,\n",
              "                                                        -5.5880e-01,  2.3389e-01, -2.1585e-01, -1.0226e-01,  4.9435e-02,\n",
              "                                                         1.6739e-01,  6.9797e-02, -1.2943e-01,  3.1545e-01, -1.4997e-01,\n",
              "                                                        -2.1408e-01,  1.0682e-01, -3.2121e-01,  1.3959e-01, -8.3104e-01,\n",
              "                                                         2.2084e-01,  1.2266e-01,  3.7062e-01, -2.3118e-01,  2.3138e-01,\n",
              "                                                         1.2532e-01,  1.1596e-01,  6.0222e-02,  1.9363e-01,  3.7853e-02,\n",
              "                                                         3.1746e-01, -8.0982e-02,  2.6098e-01,  3.9145e-01, -9.9999e-01,\n",
              "                                                        -7.6867e-02, -9.7705e-02, -1.9425e-01, -9.9999e-01, -1.6877e-01,\n",
              "                                                         1.6510e-01,  1.0897e-01,  1.8924e-01,  1.5639e-02,  2.1436e-01,\n",
              "                                                         2.6258e-01, -2.0205e-01, -1.9933e-01,  2.1862e-01, -2.7635e-01,\n",
              "                                                        -2.1669e-02,  2.4839e-01, -2.1092e-01, -1.9723e-01,  1.4701e-01,\n",
              "                                                         3.0229e-01,  3.7635e-01, -1.4870e-02, -4.7076e-01,  4.4111e-01,\n",
              "                                                        -1.0094e-01, -9.4811e-02, -4.3551e-01, -1.5859e-01,  2.5735e-01,\n",
              "                                                        -1.3036e-02,  1.0823e-01,  1.4470e-01,  3.0342e-01, -9.2594e-02,\n",
              "                                                        -2.0271e-01,  4.7906e-02, -1.3411e-01,  3.1351e-01,  1.6065e-01,\n",
              "                                                         1.6702e-01, -7.7265e-02,  4.4583e-01, -2.3032e-01, -1.3456e-01,\n",
              "                                                         4.6926e-02, -6.6956e-02,  3.8225e-01, -1.8512e-01, -1.9204e-01,\n",
              "                                                         3.7974e-01, -2.1666e-01, -6.7088e-02,  2.1756e-01,  1.3400e-01,\n",
              "                                                        -2.4468e-01, -1.2421e-01, -1.3409e-01,  3.2119e-01, -9.9999e-01,\n",
              "                                                        -5.9038e-02, -1.2530e-01,  2.2247e-01,  5.1446e-02, -1.5484e-01,\n",
              "                                                         8.1418e-01,  3.5868e-01,  3.3257e-02,  3.1401e-01, -2.9680e-01,\n",
              "                                                        -1.2265e-01, -2.9195e-01,  1.5110e-01,  8.8186e-02, -4.4364e-02,\n",
              "                                                         3.3199e-02, -1.5291e-01, -1.7318e-01]], grad_fn=<TanhBackward0>))])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "es_sent = \"maquinas de pensar\"\n",
        "tokenizer_output = mbert_tokenizer(es_sent, return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "\n",
        "mbert_model(input_ids, attention_mask = attn_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6b0d2bf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6b0d2bf6",
        "outputId": "aee87766-a6c3-4cd6-b2a7-a8c78acc6649"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions([('last_hidden_state',\n",
              "                                               tensor([[[-0.0183,  0.0577,  0.1038,  ..., -0.0349, -0.0897, -0.0034],\n",
              "                                                        [-0.2319,  0.0178, -0.0228,  ..., -0.4928,  0.1712, -0.3489],\n",
              "                                                        [-0.1214,  0.0804,  0.2790,  ..., -0.0647, -0.2073, -0.6718],\n",
              "                                                        ...,\n",
              "                                                        [-0.0835,  0.4992, -0.1798,  ...,  0.5467, -0.4407,  0.0024],\n",
              "                                                        [ 0.1764,  0.7496,  0.2607,  ...,  0.2637, -0.2617, -0.2716],\n",
              "                                                        [-0.3781,  0.5859,  0.2654,  ...,  0.3965, -0.5680, -0.8965]]],\n",
              "                                                      grad_fn=<NativeLayerNormBackward0>)),\n",
              "                                              ('pooler_output',\n",
              "                                               tensor([[ 1.9663e-01,  3.6831e-02,  1.9092e-01,  1.6675e-01,  2.1876e-01,\n",
              "                                                         4.5212e-01,  1.5738e-01, -1.8443e-01, -1.9007e-01,  2.7668e-01,\n",
              "                                                        -2.6375e-01, -2.2846e-01,  2.6193e-01, -1.7115e-01, -2.4437e-01,\n",
              "                                                         5.3468e-02,  1.8985e-01,  1.4454e-01,  2.6989e-01,  1.0231e-01,\n",
              "                                                        -7.4351e-02, -1.1409e-01,  1.3042e-01, -5.6443e-02,  3.0324e-01,\n",
              "                                                        -1.6460e-01,  2.7847e-01,  1.3497e-01,  4.5047e-01,  2.9605e-01,\n",
              "                                                         2.5428e-01,  2.3158e-01,  2.5374e-01, -7.5846e-02,  2.3163e-01,\n",
              "                                                         5.5427e-02,  1.1144e-02,  1.2426e-01,  2.6061e-01,  5.3806e-02,\n",
              "                                                         1.6303e-01,  5.1142e-01,  1.3269e-01, -1.3358e-01, -3.7102e-01,\n",
              "                                                         2.4315e-01, -1.8334e-01, -1.0740e-01,  9.9999e-01,  2.0407e-01,\n",
              "                                                         1.4211e-01, -2.0426e-01,  1.3370e-01, -3.2816e-01,  2.5923e-01,\n",
              "                                                         9.9999e-01, -3.7073e-01, -2.9902e-01,  4.3326e-02, -1.4992e-01,\n",
              "                                                        -2.3615e-01,  4.2202e-02,  3.4740e-01,  1.3696e-01, -1.5259e-01,\n",
              "                                                         7.5456e-02, -1.1221e-01,  3.4851e-01, -1.9165e-03,  9.3484e-02,\n",
              "                                                         9.4926e-02, -1.3147e-01,  2.1034e-01,  3.0040e-01, -1.8503e-01,\n",
              "                                                         8.6255e-02, -2.2896e-01, -1.7900e-01, -1.6472e-01,  1.9012e-01,\n",
              "                                                         2.1268e-01,  8.2321e-02, -2.9702e-01,  2.2585e-01, -1.8244e-01,\n",
              "                                                        -3.2864e-01, -2.8098e-01, -1.1694e-01,  1.7625e-01, -1.6935e-01,\n",
              "                                                         1.3944e-01, -1.1736e-01, -1.0675e-01, -1.4355e-01,  1.8238e-01,\n",
              "                                                        -1.6324e-01, -2.8235e-01, -8.2830e-02,  1.2819e-01, -2.6289e-01,\n",
              "                                                        -6.9057e-02,  1.9388e-01,  5.7897e-02,  4.5263e-02,  2.4417e-01,\n",
              "                                                         3.3384e-01, -2.9188e-01, -1.8632e-01, -1.7193e-02,  3.4798e-02,\n",
              "                                                        -2.2559e-01,  6.8057e-02,  4.9111e-01,  2.5987e-01,  8.5137e-02,\n",
              "                                                         3.2450e-02, -2.2121e-01, -8.0889e-01, -5.7206e-02,  1.3427e-01,\n",
              "                                                        -3.2190e-02,  9.9999e-01, -1.4184e-01, -1.3690e-01,  1.4644e-01,\n",
              "                                                        -2.6983e-01, -1.7826e-01,  2.4423e-01, -2.3707e-01,  3.6375e-01,\n",
              "                                                        -2.6699e-01, -5.4524e-02, -2.4615e-01,  6.7699e-02, -2.5381e-01,\n",
              "                                                         2.7254e-01,  1.0355e-01, -1.3467e-01,  9.7743e-02, -2.7196e-01,\n",
              "                                                         9.0038e-02,  1.4215e-01,  1.5558e-01,  1.3703e-01,  2.3534e-01,\n",
              "                                                        -2.5741e-01, -1.3028e-01, -2.0319e-03, -1.0056e-01,  2.3707e-01,\n",
              "                                                        -3.5124e-01, -2.5083e-01,  6.5748e-02, -2.3660e-01, -2.3105e-02,\n",
              "                                                        -3.2675e-01,  1.5743e-01, -3.0924e-01,  2.4876e-01,  7.8535e-02,\n",
              "                                                        -2.4270e-01,  2.4242e-01,  2.6871e-01,  1.9807e-01,  2.2575e-01,\n",
              "                                                        -2.0163e-01,  8.1418e-01, -1.9489e-01,  2.1186e-01, -2.8169e-01,\n",
              "                                                        -1.1333e-01,  1.4982e-01,  2.7974e-01,  1.9777e-01, -2.0505e-01,\n",
              "                                                        -2.9175e-01,  2.1200e-01, -7.9246e-02,  1.5474e-01, -5.4317e-01,\n",
              "                                                         1.4153e-01, -4.0566e-01, -1.5925e-01,  1.1928e-01, -2.3556e-02,\n",
              "                                                         4.5854e-02, -2.6647e-01, -7.1401e-01, -5.5387e-02, -2.4340e-01,\n",
              "                                                         1.5323e-01,  1.7945e-01,  1.5285e-01,  2.4550e-01,  2.0081e-01,\n",
              "                                                        -1.9869e-02, -2.2867e-01, -2.0114e-01, -5.2901e-02,  1.0954e-01,\n",
              "                                                        -5.2247e-01,  1.4260e-01, -3.7693e-02, -1.9527e-01, -2.8998e-01,\n",
              "                                                         2.6884e-01, -2.0809e-01,  9.9999e-01,  1.5934e-02, -2.1602e-01,\n",
              "                                                         1.5476e-01, -1.4760e-01, -4.4385e-02, -1.3111e-01, -2.0028e-01,\n",
              "                                                         1.5662e-01,  1.7127e-01, -1.2768e-01,  1.9116e-01, -2.7243e-01,\n",
              "                                                        -2.4477e-01,  7.5690e-01,  2.9928e-01, -2.4304e-01, -7.6691e-02,\n",
              "                                                        -2.8176e-01,  1.3389e-02, -3.0895e-01,  2.9156e-01, -1.8190e-01,\n",
              "                                                         9.0539e-02,  1.6665e-01,  4.5462e-01, -2.4566e-01,  1.6648e-01,\n",
              "                                                        -3.3514e-01, -2.4303e-01, -2.3925e-01, -5.1180e-01, -2.0440e-01,\n",
              "                                                         1.1086e-01,  1.6733e-01, -1.5439e-01, -4.0740e-02,  2.7723e-01,\n",
              "                                                        -2.0302e-01,  1.2149e-01, -2.1369e-01, -2.8728e-01, -1.8188e-01,\n",
              "                                                         8.6878e-02, -8.6058e-02, -1.3908e-01,  3.3000e-01,  1.6924e-01,\n",
              "                                                         1.4972e-01, -1.1761e-01, -1.9778e-01,  6.8015e-02,  2.0811e-01,\n",
              "                                                         2.1961e-01,  6.7246e-02, -1.9363e-01, -1.3662e-01, -6.5474e-03,\n",
              "                                                        -2.7672e-01,  1.5963e-01,  6.3441e-02, -1.3053e-01,  1.3130e-01,\n",
              "                                                         4.3404e-01,  1.9479e-01, -1.0431e-01,  3.6378e-01, -4.8233e-01,\n",
              "                                                         1.4847e-01,  2.2081e-01, -2.5962e-01,  2.7266e-01,  1.7587e-01,\n",
              "                                                        -4.5138e-02, -1.4823e-01,  7.9432e-02, -2.1471e-01,  8.0764e-02,\n",
              "                                                         2.2730e-01,  1.0793e-01, -1.8070e-01,  1.6334e-01,  3.5029e-01,\n",
              "                                                         1.6779e-01, -2.1697e-01, -1.6407e-01,  1.2461e-01,  2.0277e-01,\n",
              "                                                        -2.6040e-01,  3.4569e-01,  1.7667e-01, -1.8150e-01, -1.9666e-01,\n",
              "                                                         1.5590e-01, -1.9236e-01, -1.9151e-01, -1.7017e-04,  2.1024e-01,\n",
              "                                                         2.8650e-01, -1.9690e-01, -9.7160e-02, -2.0327e-01, -1.0787e-01,\n",
              "                                                         9.7428e-02, -2.1399e-01, -3.0110e-01,  2.2920e-01, -3.8049e-01,\n",
              "                                                         1.5834e-01,  2.5018e-01, -3.2356e-01, -4.5978e-02, -1.6657e-01,\n",
              "                                                         1.7146e-01,  9.9999e-01, -2.6098e-01, -1.6209e-01, -9.9999e-01,\n",
              "                                                        -4.6891e-01, -2.7055e-01,  2.7301e-01,  1.6969e-01, -3.1661e-01,\n",
              "                                                        -7.0246e-02,  1.8259e-01, -1.8207e-01, -2.7109e-01,  3.0935e-01,\n",
              "                                                         4.0538e-01, -2.0407e-01,  1.3230e-01,  1.6996e-01,  5.7928e-02,\n",
              "                                                         1.4196e-01, -9.9999e-01, -1.0303e-01,  2.0970e-02, -4.2683e-02,\n",
              "                                                        -1.0149e-01, -2.2298e-01, -2.2930e-01, -1.1753e-01,  1.3490e-01,\n",
              "                                                         2.1198e-01,  2.2512e-01, -1.0939e-01, -2.5720e-01, -3.8533e-01,\n",
              "                                                         1.8747e-01, -1.2891e-01,  6.6848e-02, -2.2334e-01, -2.9430e-01,\n",
              "                                                         6.9859e-02,  2.0419e-01,  4.1753e-02,  2.8188e-01, -1.8594e-01,\n",
              "                                                         2.6414e-01, -2.4761e-01,  9.9999e-01,  1.9128e-01, -2.0234e-01,\n",
              "                                                         9.9999e-01,  2.6540e-01, -6.9420e-02, -5.2516e-01,  1.1561e-01,\n",
              "                                                         1.1352e-01,  9.9999e-01,  1.2856e-01, -3.0153e-01, -1.6637e-01,\n",
              "                                                         1.0502e-01,  2.0734e-01,  4.9180e-01, -4.1181e-03, -9.9999e-01,\n",
              "                                                         1.0031e-01,  1.9071e-01, -5.1545e-02, -6.5302e-02,  9.9999e-01,\n",
              "                                                        -9.6873e-02, -1.9128e-01,  1.1409e-01,  3.3795e-01,  1.3597e-01,\n",
              "                                                         3.7522e-01,  2.6428e-01, -9.0982e-02, -2.0914e-01, -1.1833e-01,\n",
              "                                                        -1.1229e-01, -3.4681e-01, -2.2249e-01,  1.0144e-01, -1.6956e-01,\n",
              "                                                        -1.1350e-01, -2.2901e-01,  3.6746e-02, -4.0213e-02,  9.9999e-01,\n",
              "                                                        -1.5811e-02,  1.7973e-01,  7.8818e-02,  1.9868e-01,  9.2020e-02,\n",
              "                                                        -7.2586e-02, -6.5820e-02, -3.6706e-01,  3.4459e-01, -2.6499e-01,\n",
              "                                                         1.7574e-01,  1.1026e-01,  5.2395e-02, -2.4073e-01, -1.0504e-01,\n",
              "                                                        -2.4620e-01,  2.0411e-01,  3.9539e-02,  5.3742e-01,  2.3484e-01,\n",
              "                                                        -1.6248e-01, -1.3831e-01,  2.2213e-01, -5.6143e-02,  2.6474e-01,\n",
              "                                                        -1.8035e-01,  9.9999e-01,  2.3786e-01, -1.9461e-01, -4.1616e-02,\n",
              "                                                        -1.0487e-01, -2.5266e-01, -1.6942e-01, -2.8441e-01,  5.9210e-01,\n",
              "                                                         1.1692e-01,  8.8994e-02,  1.4863e-01,  2.7514e-01, -8.7180e-02,\n",
              "                                                        -1.1124e-01,  1.6170e-01,  6.5175e-02,  4.0325e-02, -3.3775e-01,\n",
              "                                                        -9.9999e-01, -3.0970e-01,  1.5965e-01, -4.2635e-01, -2.5099e-01,\n",
              "                                                         1.5365e-03, -3.2543e-01, -1.5290e-01, -1.8129e-02,  7.3798e-02,\n",
              "                                                         5.0980e-02, -2.6010e-01, -2.0882e-01,  1.4205e-01, -1.4203e-01,\n",
              "                                                        -2.4697e-01,  1.9888e-01,  3.1164e-01,  3.2071e-01, -8.3327e-02,\n",
              "                                                         2.7376e-01,  1.7164e-01, -1.4848e-02,  2.0021e-01, -1.1181e-01,\n",
              "                                                         8.8273e-02, -3.5306e-01, -3.8330e-01, -1.9634e-01, -2.2946e-01,\n",
              "                                                         2.1131e-01, -2.8591e-01, -7.3601e-02,  1.2059e-01, -1.9083e-01,\n",
              "                                                         2.5192e-01,  1.9797e-01, -9.7534e-02, -1.8096e-01,  7.1655e-02,\n",
              "                                                        -7.0444e-02,  1.2615e-01, -3.8281e-01,  2.4260e-01, -1.2130e-02,\n",
              "                                                         1.8283e-01, -1.9107e-01, -3.1621e-03,  1.8127e-01, -2.2065e-01,\n",
              "                                                         1.7145e-01,  1.6610e-01,  2.1985e-01, -1.2444e-01, -2.6440e-01,\n",
              "                                                         3.6380e-02,  7.6215e-02,  2.8593e-01,  9.0590e-02, -1.8787e-01,\n",
              "                                                        -1.8267e-01, -2.0696e-01, -2.7640e-01, -1.0037e-02,  2.3924e-02,\n",
              "                                                        -9.9999e-01, -2.6962e-02, -2.0045e-01, -2.2580e-01,  2.4630e-01,\n",
              "                                                         9.2688e-02,  3.3989e-01, -2.0107e-02, -5.6714e-01,  2.8104e-01,\n",
              "                                                        -1.4707e-01, -2.9393e-01,  3.2152e-02,  1.3769e-01, -4.8022e-02,\n",
              "                                                         8.6200e-02, -9.9999e-01,  6.7207e-02,  2.5256e-01, -2.3375e-01,\n",
              "                                                         1.9723e-01,  1.8645e-01, -1.2572e-01,  1.3193e-01, -1.1801e-01,\n",
              "                                                         9.1986e-01,  9.8561e-02,  1.1744e-01, -1.2444e-01, -4.9977e-03,\n",
              "                                                        -2.2342e-01, -2.9113e-01, -1.8557e-01,  1.6803e-01, -2.7972e-02,\n",
              "                                                         1.7035e-01, -2.5341e-01,  2.5812e-01,  8.0004e-02,  4.3675e-01,\n",
              "                                                         2.2243e-01, -1.0360e-01,  8.2666e-02, -1.5962e-01,  1.8148e-01,\n",
              "                                                         1.2144e-01,  3.1488e-01, -1.0175e-01, -1.0262e-02,  1.7214e-01,\n",
              "                                                        -1.6515e-01,  1.9851e-01,  1.6968e-01,  9.3980e-02,  3.4524e-01,\n",
              "                                                         2.1478e-01,  2.7286e-01, -1.2090e-01, -1.3934e-01,  1.6480e-01,\n",
              "                                                         1.2709e-01, -3.1015e-01,  7.5589e-02,  1.4997e-01, -3.8406e-02,\n",
              "                                                        -4.3522e-02,  1.2363e-01, -1.9284e-01,  1.3526e-01, -2.3024e-01,\n",
              "                                                        -2.3264e-01, -1.8424e-01,  1.6026e-01, -1.1861e-01, -3.2132e-01,\n",
              "                                                         2.4170e-01, -1.9905e-01, -2.7806e-01, -1.4937e-01, -7.5356e-03,\n",
              "                                                         2.1491e-01, -2.6794e-01, -1.5013e-01,  1.8138e-01,  3.0317e-01,\n",
              "                                                         5.7694e-02,  2.1159e-01,  1.6390e-01, -1.2928e-01,  4.6134e-02,\n",
              "                                                         2.1114e-01, -2.5208e-01, -3.5166e-01,  2.0573e-01,  1.5310e-01,\n",
              "                                                         8.7928e-02,  1.9220e-01,  6.6093e-02, -2.9686e-01,  2.3148e-01,\n",
              "                                                         8.0450e-02, -9.1040e-02, -2.0886e-01,  1.3427e-01, -1.5787e-01,\n",
              "                                                        -1.6571e-01,  1.9406e-01, -9.9999e-01, -3.6244e-03, -8.9661e-02,\n",
              "                                                        -1.3008e-01,  3.1503e-01,  2.6301e-01,  2.0833e-01,  2.2039e-01,\n",
              "                                                        -3.5182e-02, -2.4805e-01, -2.8496e-01, -3.5234e-02, -4.7605e-02,\n",
              "                                                        -5.6250e-01,  1.1065e-01,  3.8267e-02, -1.0335e-01,  2.0949e-01,\n",
              "                                                        -4.6776e-01, -2.4236e-01, -1.9264e-01, -7.9509e-02,  2.5640e-01,\n",
              "                                                         1.3585e-01,  1.1146e-01, -4.2631e-01, -1.3797e-01, -4.2020e-02,\n",
              "                                                        -3.3589e-01, -3.2083e-01,  2.8107e-01,  2.4660e-01,  2.7439e-01,\n",
              "                                                        -6.5341e-01,  2.4811e-01, -2.2557e-01, -1.0728e-01,  1.3671e-01,\n",
              "                                                         1.7659e-01,  5.3338e-02, -1.9266e-01,  3.2107e-01, -1.6023e-01,\n",
              "                                                        -2.1777e-01,  1.0947e-01, -3.1034e-01,  1.6971e-01, -8.8409e-01,\n",
              "                                                         2.5713e-01,  1.5591e-01,  3.4103e-01, -2.2164e-01,  2.5962e-01,\n",
              "                                                         1.5191e-01,  1.2868e-01,  6.3957e-02,  2.0855e-01,  2.8089e-02,\n",
              "                                                         3.6860e-01, -1.4804e-01,  2.7331e-01,  4.2737e-01, -9.9999e-01,\n",
              "                                                        -1.2161e-01, -1.1271e-01, -2.3094e-01, -9.9999e-01, -1.4309e-01,\n",
              "                                                         1.6571e-01,  8.9037e-02,  2.0814e-01, -3.1244e-02,  2.2422e-01,\n",
              "                                                         3.3944e-01, -2.1243e-01, -2.2818e-01,  2.4313e-01, -2.5976e-01,\n",
              "                                                        -3.0441e-02,  2.5024e-01, -2.2388e-01, -1.9171e-01,  1.5171e-01,\n",
              "                                                         3.2445e-01,  4.3148e-01, -3.2403e-02, -5.4172e-01,  4.6540e-01,\n",
              "                                                        -1.2867e-01, -1.2872e-01, -4.5294e-01, -1.9522e-01,  2.7236e-01,\n",
              "                                                        -2.0256e-05,  1.6263e-01,  1.8271e-01,  2.9891e-01, -9.6923e-02,\n",
              "                                                        -2.4060e-01,  5.9647e-02, -1.2879e-01,  3.1317e-01,  1.3269e-01,\n",
              "                                                         1.5556e-01, -5.0566e-02,  5.3613e-01, -2.8093e-01, -1.4543e-01,\n",
              "                                                         3.8241e-02, -2.9764e-02,  4.5187e-01, -1.4331e-01, -2.3235e-01,\n",
              "                                                         3.9609e-01, -2.5692e-01, -1.0771e-01,  2.4073e-01,  1.2628e-01,\n",
              "                                                        -2.6284e-01, -1.0109e-01, -1.2280e-01,  3.4987e-01, -9.9999e-01,\n",
              "                                                        -8.0383e-02, -1.3164e-01,  2.1572e-01,  1.7950e-02, -1.7570e-01,\n",
              "                                                         8.7421e-01,  4.5491e-01,  8.7202e-02,  2.9448e-01, -3.2925e-01,\n",
              "                                                        -1.3678e-01, -3.7717e-01,  1.4726e-01,  1.3700e-01, -5.8430e-02,\n",
              "                                                         2.6105e-02, -1.6864e-01, -1.6733e-01]], grad_fn=<TanhBackward0>))])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hi_sent = \"सोच मशीन\"\n",
        "tokenizer_output = mbert_tokenizer(hi_sent, return_tensors=\"pt\")\n",
        "input_ids, attn_mask = tokenizer_output[\"input_ids\"], tokenizer_output[\"attention_mask\"]\n",
        "\n",
        "mbert_model(input_ids, attention_mask = attn_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5877901b",
      "metadata": {
        "id": "5877901b"
      },
      "source": [
        "Hence, we can very easily use mBERT for generating predictions on texts written in different languages."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b6f23d0",
      "metadata": {
        "id": "6b6f23d0"
      },
      "source": [
        "## Fine-tune mBERT on XNLI\n",
        "\n",
        "We can now start fine-tuning mBERT on this dataset. We will start by defining the custom `Dataset` class for the task and then define the model and training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45bf6240",
      "metadata": {
        "id": "45bf6240"
      },
      "source": [
        "## Custom Dataset Class\n",
        "\n",
        "Implement the `XNLImBertDataset` class below that processes and stores the data as well as provides a way to iterate through the dataset. The details about various methods in the class are mentioned in their docstrings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "984d8f79",
      "metadata": {
        "id": "984d8f79"
      },
      "outputs": [],
      "source": [
        "class XNLImBertDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, premises,\n",
        "                 hypotheses,\n",
        "                 labels,\n",
        "                 max_length,\n",
        "                mbert_variant = \"bert-base-multilingual-uncased\"):\n",
        "        \n",
        "        \"\"\"\n",
        "        Constructor for the `XNLImBertDataset` class. Stores the `premises`, `hypotheses` and `labels`\n",
        "        which can then be used by other methods. Also initializes the tokenizer.\n",
        "        \n",
        "        Inputs:\n",
        "            - premises (list) : A list of sentences constituting the premise in each example\n",
        "            - hypotheses (list) : A list of sentences constituting the hypothesis in each example\n",
        "            - labels (list) : A list of labels denoting for each premise-hypothesis pair.\n",
        "            - max_length (int): Maximum length of the encoded sequence.  \n",
        "                                If number of tokens are lower than `max_length` add padding otherwise truncate\n",
        "        \n",
        "        \n",
        "        Note that labels are in the form of strings \"entailment\", \"contradiction\" and \"neutral\". For training the\n",
        "        models we will want the labels in the numeric form, so you should define a mapping from the text label\n",
        "        to a numeric id. You should order the labels in alphabetical order while defining the mapping i.e. \n",
        "        contradiction -> 0, entailment -> 1, \"neutral\" - > 2 (such that we have consistency across everyone) \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        self.premises = None\n",
        "        self.hypotheses = None\n",
        "        self.labels = None\n",
        "        self.max_length = None\n",
        "        self.tokenizer = None\n",
        "        self.label2id = None # Define it as a dictionary\n",
        "        \n",
        "        mbert_tokenizer = BertTokenizer.from_pretrained(mbert_variant)\n",
        "\n",
        "        self.premises = premises\n",
        "        self.hypotheses = hypotheses\n",
        "        self.labels = labels\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = mbert_tokenizer\n",
        "        self.label2id = { \"contradiction\" : 0, \"entailment\" : 1, \"neutral\" : 2 }\n",
        "        \n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the dataset\n",
        "        \"\"\"\n",
        "        length = None\n",
        "        \n",
        "        length = len(self.premises)\n",
        "        \n",
        "        return length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        \n",
        "        Returns the features and label corresponding to the the `idx` entry in the dataset.\n",
        "        \n",
        "        Inputs:\n",
        "            - idx (int): Index corresponding to the sentence_pair,label to be returned\n",
        "        \n",
        "        Returns:\n",
        "            - input_ids (torch.tensor): Indices of the tokens in the sentence pair.\n",
        "                                        Shape of the tensor should be (`seq_len`,)\n",
        "            - mask (torch.tensor): Attention mask indicating which tokens are padded.\n",
        "            - label (int): Label for the premise-hypothesis pair\n",
        "    \n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        input_ids = None\n",
        "        mask = None\n",
        "        label = None\n",
        "        \n",
        "        premise  = self.premises[idx]\n",
        "        hypothesis = self.hypotheses[idx]\n",
        "        sentence = premise + \"[SEP]\" + hypothesis\n",
        "        tokenized = mbert_tokenizer(sentence, max_length=self.max_length, padding=\"max_length\", truncation = True, return_tensors=\"pt\")\n",
        "        input_ids = tokenized[\"input_ids\"]\n",
        "        mask = tokenized[\"attention_mask\"]\n",
        "        label_init = self.labels[idx]\n",
        "        if label_init == \"contradiction\":\n",
        "          label = 0\n",
        "        elif label_init == \"entailment\":\n",
        "          label = 1\n",
        "        elif label_init == \"neutral\":\n",
        "          label = 2\n",
        "        \n",
        "        return input_ids.squeeze(0), mask.squeeze(0), label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4e1ba52b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4e1ba52b",
        "outputId": "bc49b47b-a458-463c-e3a1-1eea3357fc62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Sample Test Cases\n",
            "Sample Test Case 1: Checking if `__len__` is implemented correctly\n",
            "Dataset Length: 3\n",
            "Expected Length: 3\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\n",
            "input_ids:\n",
            " tensor([  101,   143, 10564, 15450, 84789, 10107, 10103, 38884, 10108,   143,\n",
            "        16745, 10104, 10970, 11344, 17147, 11913,   119,   102, 10103, 10564,\n",
            "        10127, 55860,   119,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "Expected input_ids:\n",
            " tensor([  101,   143, 10564, 15450, 84789, 10107, 10103, 38884, 10108,   143,\n",
            "        16745, 10104, 10970, 11344, 17147, 11913,   119,   102, 10103, 10564,\n",
            "        10127, 55860,   119,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "label:\n",
            " 0\n",
            "Expected label:\n",
            " 0\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\n",
            "input_ids:\n",
            " tensor([  101, 10144, 18585, 10110, 24392, 10564, 14965, 64581,   119,   102,\n",
            "        10536, 10562, 10320, 14965, 64581, 10110, 18418, 82863, 10160, 10103,\n",
            "        45670, 14734, 10125, 10103, 21005,   119,   102,     0,     0,     0,\n",
            "            0,     0])\n",
            "Expected input_ids:\n",
            " tensor([  101, 10144, 18585, 10110, 24392, 10564, 14965, 64581,   119,   102,\n",
            "        10536, 10562, 10320, 14965, 64581, 10110, 18418, 82863, 10160, 10103,\n",
            "        45670, 14734, 10125, 10103, 21005,   119,   102,     0,     0,     0,\n",
            "            0,     0])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0])\n",
            "label:\n",
            " 2\n",
            "Expected label:\n",
            " 2\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 2`\n",
            "input_ids:\n",
            " tensor([  101,   143, 20071, 11336, 10171, 18248, 19592, 14734,   119,   102,\n",
            "        10970, 10562, 10320, 14734,   143, 13148,   119,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "Expected input_ids:\n",
            " tensor([  101,   143, 20071, 11336, 10171, 18248, 19592, 14734,   119,   102,\n",
            "        10970, 10562, 10320, 14734,   143, 13148,   119,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "label:\n",
            " 1\n",
            "Expected label:\n",
            " 1\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n",
            "Sample Test Case 5: Checking for hindi\n",
            "input_ids:\n",
            " tensor([  101, 11384,   569, 30119, 10949, 11142, 74535, 10949,   533, 13764,\n",
            "        25695,   571, 12114, 19086, 10949, 36335,   580,   591,   102,   568,\n",
            "        11551, 17109, 12334, 56426, 52061,   569, 28393, 41790, 20106, 11483,\n",
            "        91329, 19086, 29931,   533, 13764,   102])\n",
            "Expected input_ids:\n",
            " tensor([  101, 11384,   569, 30119, 10949, 11142, 74535, 10949,   533, 13764,\n",
            "        25695,   571, 12114, 19086, 10949, 36335,   580,   591,   102,   568,\n",
            "        11551, 17109, 12334, 56426, 52061,   569, 28393, 41790, 20106, 11483,\n",
            "        91329, 19086, 29931,   533, 13764,   102])\n",
            "mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "Expected mask:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "label:\n",
            " 2\n",
            "Expected label:\n",
            " 2\n",
            "Sample Test Case Passed!\n",
            "****************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Sample Test Cases\")\n",
        "sample_premises = [\"A man inspects the uniform of a figure in some East Asian country.\",\n",
        "                    \"An older and younger man smiling.\",\n",
        "                   \"A soccer game with multiple males playing.\"\n",
        "                    ]\n",
        "sample_hypotheses = [\"The man is sleeping.\",\n",
        "                     \"Two men are smiling and laughing at the cats playing on the floor.\",\n",
        "                    \"Some men are playing a sport.\"]\n",
        "sample_labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
        "sample_max_len = 32\n",
        "sample_dataset = XNLImBertDataset(\n",
        "    sample_premises,\n",
        "    sample_hypotheses,\n",
        "    sample_labels,\n",
        "    sample_max_len\n",
        ")\n",
        "print(f\"Sample Test Case 1: Checking if `__len__` is implemented correctly\")\n",
        "dataset_len= len(sample_dataset)\n",
        "expected_len = len(sample_labels)\n",
        "print(f\"Dataset Length: {dataset_len}\")\n",
        "print(f\"Expected Length: {expected_len}\")\n",
        "assert len(sample_dataset) == len(sample_premises)\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 2: Checking if `__getitem__` is implemented correctly for `idx= 0`\")\n",
        "sample_idx = 0\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids =  torch.tensor([  101,   143, 10564, 15450, 84789, 10107, 10103, 38884, 10108,   143,\n",
        "        16745, 10104, 10970, 11344, 17147, 11913,   119,   102, 10103, 10564,\n",
        "        10127, 55860,   119,   102,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0])\n",
        "expected_label = 0\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 3: Checking if `__getitem__` is implemented correctly for `idx= 1`\")\n",
        "sample_idx = 1\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids = torch.tensor([  101, 10144, 18585, 10110, 24392, 10564, 14965, 64581,   119,   102,\n",
        "        10536, 10562, 10320, 14965, 64581, 10110, 18418, 82863, 10160, 10103,\n",
        "        45670, 14734, 10125, 10103, 21005,   119,   102,     0,     0,     0,\n",
        "            0,     0])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 0, 0, 0, 0, 0])\n",
        "expected_label = 2\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "\n",
        "print(f\"Sample Test Case 4: Checking if `__getitem__` is implemented correctly for `idx= 2`\")\n",
        "sample_idx = 2\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids = torch.tensor([  101,   143, 20071, 11336, 10171, 18248, 19592, 14734,   119,   102,\n",
        "        10970, 10562, 10320, 14734,   143, 13148,   119,   102,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0])\n",
        "expected_label = 1\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "\n",
        "\n",
        "sample_premises = [\"एक आदमी किसी पूर्वी एशियाई देश में एक आकृति की वर्दी का निरीक्षण करता है।\",\n",
        "                    \"एक बूढ़ा और छोटा आदमी मुस्कुरा रहा है।\",\n",
        "                   \"एक फ़ुटबॉल खेल जिसमें कई पुरुष खेल रहे हैं।\"\n",
        "                    ]\n",
        "sample_sentence2s = [\"आदमी सो रहा है।\",\n",
        "                     \"फर्श पर खेल रही बिल्लियों को देखकर दो आदमी मुस्कुरा रहे हैं और हंस रहे हैं।\",\n",
        "                    \"कुछ पुरुष कोई खेल खेल रहे हैं।\"\n",
        "                    ]\n",
        "sample_labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
        "sample_max_len = 36\n",
        "sample_dataset = XNLImBertDataset(\n",
        "    sample_premises,\n",
        "    sample_sentence2s,\n",
        "    sample_labels,\n",
        "    sample_max_len\n",
        ")\n",
        "\n",
        "print(f\"Sample Test Case 5: Checking for hindi\")\n",
        "sample_idx = 1\n",
        "input_ids, mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "expected_input_ids =  torch.tensor([  101, 11384,   569, 30119, 10949, 11142, 74535, 10949,   533, 13764,\n",
        "        25695,   571, 12114, 19086, 10949, 36335,   580,   591,   102,   568,\n",
        "        11551, 17109, 12334, 56426, 52061,   569, 28393, 41790, 20106, 11483,\n",
        "        91329, 19086, 29931,   533, 13764,   102])\n",
        "expected_mask = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
        "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
        "expected_label = 2\n",
        "print(f\"input_ids:\\n {input_ids}\")\n",
        "print(f\"Expected input_ids:\\n {expected_input_ids}\")\n",
        "assert (expected_input_ids == input_ids).all()\n",
        "\n",
        "print(f\"mask:\\n {mask}\")\n",
        "print(f\"Expected mask:\\n {expected_mask}\")\n",
        "assert (expected_mask == mask).all()\n",
        "\n",
        "print(f\"label:\\n {label}\")\n",
        "print(f\"Expected label:\\n {expected_label}\")\n",
        "assert expected_label == label\n",
        "\n",
        "print(\"Sample Test Case Passed!\")\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3863fe4d",
      "metadata": {
        "id": "3863fe4d"
      },
      "source": [
        "Initialize dataset and dataloaders for english training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "80913533",
      "metadata": {
        "id": "80913533"
      },
      "outputs": [],
      "source": [
        "max_seq_len = 128\n",
        "batch_size = 8\n",
        "\n",
        "train_en_premises, train_en_hypotheses = train_en_data[\"premise\"].values, train_en_data[\"hypothesis\"].values\n",
        "train_en_labels = train_en_data[\"label\"].values\n",
        "\n",
        "val_en_premises, val_en_hypotheses = val_en_data[\"premise\"].values, val_en_data[\"hypothesis\"].values\n",
        "val_en_labels = val_en_data[\"label\"].values\n",
        "\n",
        "train_en_dataset = XNLImBertDataset(train_en_premises, train_en_hypotheses, train_en_labels, max_seq_len)\n",
        "val_en_dataset = XNLImBertDataset(val_en_premises, val_en_hypotheses, val_en_labels, max_seq_len)\n",
        "\n",
        "train_en_dataloader = DataLoader(train_en_dataset, batch_size = batch_size)\n",
        "val_en_dataloader = DataLoader(val_en_dataset, batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ca63c08",
      "metadata": {
        "id": "5ca63c08"
      },
      "source": [
        "## Implement mBERT Based Classifier for NLI\n",
        "\n",
        "We need to use use Sigmoid function in the output layer and instead of Softmax function because we have 3 classes not 2. \n",
        "we will get 3 numbers as output for each input denoting the probability of each of the 3 classes. Also, it is common to use Log of the Softmax function instead of plain softmax to obtain log-probabilities. Log-Softmax is numerically more stable and hence it is often more used in practice. We can read about it's usage in pytorch [here](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "OkazGuCaazBp",
      "metadata": {
        "id": "OkazGuCaazBp"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "874c8194",
      "metadata": {
        "id": "874c8194"
      },
      "outputs": [],
      "source": [
        "\n",
        "class mBERTNLIClassifierModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_hidden = 768, mbert_variant = \"bert-base-multilingual-uncased\"):\n",
        "        \n",
        "        \"\"\"\n",
        "        Constructor for the `mBERTNLIClassifierModel` class. Use this to define  the network architecture\n",
        "        which should be: Input -> mBERT -> Linear Layer -> Log-Softmax\n",
        "        \n",
        "        Inputs:\n",
        "            - d_hidden (int): Size of the hidden representations of mbert\n",
        "            - mbert_variant (str): mBERT variant to use\n",
        "        \n",
        "        \"\"\"\n",
        "        super(mBERTNLIClassifierModel, self).__init__()\n",
        "        \n",
        "        self.mbert_layer = None\n",
        "        self.output_layer = None\n",
        "        self.log_softmax_layer = None\n",
        "        \n",
        "        self.mbert_layer = BertModel.from_pretrained(mbert_variant)\n",
        "        self.output_layer = nn.Linear(d_hidden,3)\n",
        "        self.log_softmax_layer = nn.LogSoftmax()\n",
        "        \n",
        "        \n",
        "    def forward(self, input_ids, attn_mask):\n",
        "        \n",
        "        \"\"\"\n",
        "        Forward Passes the inputs through the network and obtains the prediction\n",
        "        \n",
        "        Inputs:\n",
        "            - input_ids (torch.tensor): A torch tensor of shape [batch_size, seq_len]\n",
        "                                        representing the sequence of token ids\n",
        "            - attn_mask (torch.tensor): A torch tensor of shape [batch_size, seq_len]\n",
        "                                        representing the attention mask such that padded tokens are 0 and rest 1\n",
        "                                        \n",
        "        Returns:\n",
        "          - output (torch.tensor): A torch tensor of shape [batch_size, 3] containing (log) probabilities\n",
        "          of each class \n",
        "                                                \n",
        "        \"\"\"\n",
        "        \n",
        "        output = None\n",
        "        \n",
        "        output = self.mbert_layer(input_ids, attention_mask = attn_mask, output_attentions=True)\n",
        "        pooler_output = output.pooler_output\n",
        "\n",
        "        output = self.log_softmax_layer(self.output_layer(pooler_output))\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "06229181",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "06229181",
        "outputId": "98398ef4-51a9-4e56-a3a7-72a9d0cccd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Sample Test Cases!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Test Case 1\n",
            "Model Output: [[-0.9885042 -1.479876  -0.9157878]]\n",
            "Expected Output: [[-0.9885041 -1.479876  -0.915788 ]]\n",
            "Test Case Passed! :)\n",
            "******************************\n",
            "\n",
            "Sample Test Case 2\n",
            "Model Output: [[-0.97441864 -1.477538   -0.9304163 ]]\n",
            "Expected Output: [[-0.97441876 -1.4775381  -0.9304163 ]]\n",
            "Test Case Passed! :)\n",
            "******************************\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Running Sample Test Cases!\")\n",
        "torch.manual_seed(42)\n",
        "model = mBERTNLIClassifierModel()\n",
        "\n",
        "sample_premises = [\"A man inspects the uniform of a figure in some East Asian country.\",\n",
        "                    \"An older and younger man smiling.\",\n",
        "                   \"A soccer game with multiple males playing.\"\n",
        "                    ]\n",
        "sample_hypotheses = [\"The man is sleeping.\",\n",
        "                     \"Two men are smiling and laughing at the cats playing on the floor.\",\n",
        "                    \"Some men are playing a sport.\"]\n",
        "sample_labels = [\"contradiction\", \"neutral\", \"entailment\"]\n",
        "sample_max_len = 32\n",
        "sample_dataset = XNLImBertDataset(\n",
        "    sample_premises,\n",
        "    sample_hypotheses,\n",
        "    sample_labels,\n",
        "    sample_max_len\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Sample Test Case 1\")\n",
        "sample_idx = 0\n",
        "input_ids, attn_mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "mbert_cls_out = model(input_ids.unsqueeze(0), attn_mask.unsqueeze(0)).detach().numpy()\n",
        "expected_mbert_cls_out = np.array([[-0.9885041, -1.479876,  -0.915788 ]])\n",
        "print(f\"Model Output: {mbert_cls_out }\")\n",
        "print(f\"Expected Output: {expected_mbert_cls_out}\")\n",
        "\n",
        "assert mbert_cls_out .shape == expected_mbert_cls_out.shape\n",
        "assert np.allclose(mbert_cls_out, expected_mbert_cls_out, 1e-4)\n",
        "print(\"Test Case Passed! :)\")\n",
        "print(\"******************************\\n\")\n",
        "\n",
        "print(\"Sample Test Case 2\")\n",
        "sample_idx = 1\n",
        "input_ids, attn_mask, label = sample_dataset.__getitem__(sample_idx)\n",
        "mbert_cls_out = model(input_ids.unsqueeze(0), attn_mask.unsqueeze(0)).detach().numpy()\n",
        "expected_mbert_cls_out = np.array([[-0.97441876, -1.4775381,  -0.9304163 ]])\n",
        "print(f\"Model Output: {mbert_cls_out }\")\n",
        "print(f\"Expected Output: {expected_mbert_cls_out}\")\n",
        "\n",
        "assert mbert_cls_out .shape == expected_mbert_cls_out.shape\n",
        "assert np.allclose(mbert_cls_out, expected_mbert_cls_out, 1e-4)\n",
        "print(\"Test Case Passed! :)\")\n",
        "print(\"******************************\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30c8ca8f",
      "metadata": {
        "id": "30c8ca8f"
      },
      "source": [
        "## Training and Evaluating the Model\n",
        "\n",
        "We will use the [Negative Log-Likelihood Loss function](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html). While evaluating the accuracy for 3 classes, it is common to predict the class as the label which has the highest probability (or equivalently log probability).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "787c97cf",
      "metadata": {
        "id": "787c97cf"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_dataloader, device = \"cpu\"):\n",
        "    \n",
        "    \"\"\"\n",
        "    Evaluates `model` on test dataset\n",
        "\n",
        "    Inputs:\n",
        "        - model (mBERTNLIClassifierModel): mBERT based classifier model to be evaluated\n",
        "        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n",
        "\n",
        "    Returns:\n",
        "        - accuracy (float): Average accuracy over the test dataset \n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    accuracy = None\n",
        "    \n",
        "    model.eval()\n",
        "    model = model.to(device)\n",
        "    accuracy = 0\n",
        "    \n",
        "    # by specifying `torch.no_grad`, it ensures no gradients are calcuated while running the model,\n",
        "    # this makes the computation much more faster\n",
        "    with torch.no_grad():\n",
        "      for test_batch in test_dataloader:\n",
        "        input_ids, attn_mask, labels = test_batch\n",
        "        input_ids = input_ids.to(device).long()\n",
        "        attn_mask = attn_mask.to(device).long()\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Step 1: Get probability predictions from the model and store it in `pred_probs`\n",
        "        pred_probs = model(input_ids, attn_mask)\n",
        "        pred_probs = torch.argmax(pred_probs, dim=1)\n",
        "\n",
        "        # Convert predictions and labels to numpy arrays from torch tensors as they are easier to operate for computing metrics\n",
        "        # pred_probs = pred_probs.cpu().detach().numpy()\n",
        "        # labels = labels.cpu().detach().numpy()\n",
        "        # Step 2: Get accuracy of predictions and store it in `batch_accuracy`\n",
        "        batch_accuracy = (labels == pred_probs).sum() / len(labels)\n",
        "        accuracy += batch_accuracy\n",
        "\n",
        "      # Divide by number of batches to get average accuracy\n",
        "      accuracy = accuracy / len(test_dataloader)\n",
        "    \n",
        "    return accuracy\n",
        "    \n",
        "    \n",
        "    \n",
        "def train(model, train_dataloader, val_dataloader,\n",
        "          lr = 1e-5, num_epochs = 3,\n",
        "          device = \"cpu\"):\n",
        "    \n",
        "    \"\"\"\n",
        "    Runs the training loop. Define the loss function as NLLLoss\n",
        "    and optimizer as Adam and train for `num_epochs` epochs.\n",
        "\n",
        "    Inputs:\n",
        "        - model (mBERTNLIClassifierModel): mBERT based classifer model to be trained\n",
        "        - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n",
        "        - val_dataloader (torch.utils.DataLoader): A dataloader defined over the validation dataset\n",
        "        - lr (float): The learning rate for the optimizer\n",
        "        - num_epochs (int): Number of epochs to train the model for.\n",
        "        - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        - best_model (mBERTNLIClassifierModel): model corresponding to the highest validation accuracy (checked at the end of each epoch)\n",
        "        - best_val_accuracy (float): Validation accuracy corresponding to the best epoch\n",
        "    \"\"\"\n",
        "        \n",
        "    best_val_accuracy = float(\"-inf\")\n",
        "    best_model = None\n",
        "    \n",
        "    \n",
        "    loss_fn = None\n",
        "    optimizer = None\n",
        "    \n",
        "    loss_fn = nn.NLLLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        \n",
        "        for train_batch in tqdm.tqdm(train_dataloader):\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          input_ids, attn_mask = train_batch[0], train_batch[1]\n",
        "          input_ids = input_ids.to(device)\n",
        "          attn_mask = attn_mask.to(device)\n",
        "          preds = model(input_ids, attn_mask)\n",
        "          target = train_batch[2].to(device)\n",
        "          loss = loss_fn(preds, target)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          epoch_loss += loss.item()\n",
        "        \n",
        "        epoch_loss = epoch_loss / len(train_dataloader)\n",
        "        \n",
        "        val_accuracy = 0\n",
        "        val_accuracy = evaluate(model, val_dataloader, device=device)\n",
        "        \n",
        "        # Model selection\n",
        "        if val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "            best_model = copy.deepcopy(model) \n",
        "    best_model.zero_grad()\n",
        "    \n",
        "    return best_model, best_val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "5c77e27c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5c77e27c",
        "outputId": "cdef610f-0c1b-4de7-961c-42089f69c9d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on 100 data points for sanity check\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/13 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "100%|██████████| 13/13 [01:17<00:00,  5.96s/it]\n",
            "100%|██████████| 13/13 [01:11<00:00,  5.47s/it]\n",
            "100%|██████████| 13/13 [01:08<00:00,  5.30s/it]\n",
            "100%|██████████| 13/13 [01:09<00:00,  5.38s/it]\n",
            "100%|██████████| 13/13 [01:10<00:00,  5.40s/it]\n",
            "100%|██████████| 13/13 [01:08<00:00,  5.29s/it]\n",
            "100%|██████████| 13/13 [01:08<00:00,  5.30s/it]\n",
            "100%|██████████| 13/13 [01:10<00:00,  5.40s/it]\n",
            "100%|██████████| 13/13 [01:10<00:00,  5.42s/it]\n",
            "100%|██████████| 13/13 [01:08<00:00,  5.30s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Validation Accuracy: 0.9807692170143127\n",
            "Expected Best Validation Accuracy: 0.99\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "print(\"Training on 100 data points for sanity check\")\n",
        "\n",
        "max_seq_len = 128\n",
        "batch_size = 8\n",
        "\n",
        "sample_premises, sample_hypotheses = train_en_data[\"premise\"].values[:100], train_en_data[\"hypothesis\"].values[:100]\n",
        "sample_labels = train_en_data[\"label\"].values[:100]\n",
        "\n",
        "sample_dataset = XNLImBertDataset(sample_premises, sample_hypotheses, sample_labels, max_seq_len)\n",
        "sample_dataloader = DataLoader(sample_dataset, batch_size = batch_size)\n",
        "\n",
        "\n",
        "model = mBERTNLIClassifierModel()\n",
        "best_model, best_val_acc = train(model, sample_dataloader, sample_dataloader, lr = 5e-5, num_epochs = 10, device = \"cpu\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc}\")\n",
        "print(f\"Expected Best Validation Accuracy: {0.99}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a628b8d9",
      "metadata": {
        "id": "a628b8d9"
      },
      "source": [
        "Since we just trained and evaluated on same 100 examples, you should expect nearly perfect 99% accuracy. Now let's train on the entire dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8d168c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a8d168c4",
        "outputId": "688f964a-6697-4701-8b92-8597e909850c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "  0%|          | 0/4750 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  1%|          | 25/4750 [02:37<7:38:43,  5.83s/it]"
          ]
        }
      ],
      "source": [
        "model = mBERTNLIClassifierModel()\n",
        "best_model, best_val_acc = train(model, train_en_dataloader, val_en_dataloader, lr = 1e-5, num_epochs = 2, device = \"cpu\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc}\")\n",
        "print(f\"Expected Best Validation Accuracy: {0.7675}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e700e982",
      "metadata": {
        "id": "e700e982"
      },
      "source": [
        "## Zero-Shot Transfer\n",
        "\n",
        "Pre-trained multilingual models like mBERT have shown to exhibit zero-shot transfer capabilities to new langauges for which the model was never fine-tuned on. You can read more about zero-shot transfer in mBERT in this [paper](https://arxiv.org/abs/1906.01502). We will evaluate the performance of the mBERT classifier that we just trained on the English on the test sets in 15 different languages. Ithe `evaluate_on_diff_langs` function below does just that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dcad1a6",
      "metadata": {
        "id": "7dcad1a6"
      },
      "outputs": [],
      "source": [
        "def evaluate_on_diff_langs(model, lang2test_df, max_length = 128, batch_size = 8, device = \"cpu\"):\n",
        "    \n",
        "    \"\"\"\n",
        "    Evaluates the accuracy of the fine-tuned model on test data in different langauges.\n",
        "    \n",
        "    Inputs:\n",
        "        - model (mBERTNLIClassifierModel): mBERT based classifer model fine-tuned on English data\n",
        "        - lang2test_df (dict): A dictionary with langauges as keys and\n",
        "                                their corresponding test sets (in form of pandas dataframe)\n",
        "                                as values\n",
        "                                \n",
        "    Returns:\n",
        "        - lang2acc (dict): A dictionary with language ids as keys and the accuracy on it's test set as values\n",
        "                            eg: {\"en\" : 0.8, \"fr\" : 0.77, \"hi\": 0.72, ...}\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    lang2acc = None\n",
        "\n",
        "    lang2acc = {  lang : evaluate(model, lang2test_df[lang], device = \"cpu\") for lang in test_langs}\n",
        "    \n",
        "    return lang2acc\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39ff6b5f",
      "metadata": {
        "id": "39ff6b5f"
      },
      "outputs": [],
      "source": [
        "lang2acc = evaluate_on_diff_langs(best_model, lang2test_df, max_length = 128, batch_size = 8, device = \"cuda\")\n",
        "expected_vals = {'ar': 0.5989583333333334,\n",
        " 'bg': 0.6454326923076923,\n",
        " 'de': 0.6698717948717948,\n",
        " 'el': 0.6402243589743589,\n",
        " 'en': 0.7263621794871795,\n",
        " 'es': 0.6923076923076923,\n",
        " 'fr': 0.6802884615384616,\n",
        " 'hi': 0.5893429487179487,\n",
        " 'ru': 0.6478365384615384,\n",
        " 'sw': 0.53125,\n",
        " 'th': 0.35136217948717946,\n",
        " 'tr': 0.610176282051282,\n",
        " 'ur': 0.5637019230769231,\n",
        " 'vi': 0.6193910256410257,\n",
        " 'zh': 0.6073717948717948}\n",
        "print(f\"Langauge to Accuracy:\\n {lang2acc}\")\n",
        "print(f\"Expected Values:\\n {expected_vals}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65558d2f",
      "metadata": {
        "id": "65558d2f"
      },
      "source": [
        "The values often do not match exactly, but we can expect similar patterns i.e. the fine-tuned model on English data, performs reasonably on other new langauges as well compared to it's performance on English test data. Performance on langauges like German, French and Spanish is much closer to the performance on English. However, it is on the lower side for languages like Swahilli, Urdu and Thai. The values are still surprisingly high, considering a random guess will fetch us an accuracy of 33%."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Assignment4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
